{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Global LLM Instance\n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "import json\n",
    "from inspect import signature\n",
    "\n",
    "try: GLOBAL_LLM_INSTANCE\n",
    "except NameError:\n",
    "    print('Initializing Global LLM Instance')\n",
    "    GLOBAL_LLM_INSTANCE = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM:\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    * Adapt this to more types of underlying models (mistral, openai, etc)\n",
    "    * Higher forms of memory\n",
    "    * observation\n",
    "    * reflection\n",
    "    * etc\n",
    "    '''\n",
    "\n",
    "    def __init__(self, system_prompt:str='', temperature=0.1, repeat_penalty=1.05):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        \"\"\"\n",
    "        Create or use a global LLM instance, and initialize history\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - <see LLM.reset>\n",
    "        temperature:float - <see LLM.reset>\n",
    "        repeat_penalty:float - <see LLM.reset>\n",
    "        \"\"\"\n",
    "        if GLOBAL_LLM_INSTANCE == None:\n",
    "            GLOBAL_LLM_INSTANCE = llama_cpp.Llama(model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf', n_gpu_layers=-1, verbose=0, n_ctx=4000)\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    def __call__(self, prompt:str='', role:str='user', response_format:dict=None):\n",
    "        \"\"\"\n",
    "        Elicit a response from the LLM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt:str - text to append to the history before eliciting a response, or an empty string to use the existing history without adding to it\n",
    "        role:str - the role associated with the prompt text: 'user', 'system', or 'assistant'. Ignored if prompt is None.\n",
    "        response_format:dict - a dict format to force the response to be in -- e.g., `{'to': '<who you are talking to>', 'response': '<your actual response>'}` -- or `None` for the response to be a regular string\n",
    "        \"\"\"\n",
    "\n",
    "        if response_format:\n",
    "            self._history += [{\n",
    "                'role':'user',\n",
    "                'content': 'Your next output should be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "            }]\n",
    "\n",
    "        if prompt:\n",
    "            self._history += [{'role':role, 'content':prompt}]\n",
    "\n",
    "        last_msg_idx = len(self._history)\n",
    "\n",
    "        resp = self._force_chat_completion()\n",
    "        resp_dict = None\n",
    "\n",
    "        if response_format:\n",
    "            while True:\n",
    "                try:\n",
    "                    if '}' not in resp:\n",
    "                        resp += '}'\n",
    "                    resp = resp[resp.index('{'):] # the json might be surounded by other text\n",
    "                    resp_dict = json.loads(resp)\n",
    "                    break\n",
    "                except:\n",
    "                    self._history += [{\n",
    "                        'role':'user',\n",
    "                        'content': 'Your previous output WAS NOT correctly formatted. Make sure it has necessary curly brackets and quotes. It shold be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "                    }]\n",
    "                    resp = self._force_chat_completion()\n",
    "\n",
    "                # for debug:\n",
    "                # clear_output(wait=True)\n",
    "                # print(self.get_hist())\n",
    "                print('bad json:', resp)\n",
    "\n",
    "            # remove correction messages\n",
    "            self._history = (\n",
    "                self._history[0:last_msg_idx-2] + # up to format prompt\n",
    "                self._history[last_msg_idx-1:last_msg_idx] + # user prompt\n",
    "                self._history[-1:] # final response\n",
    "            )\n",
    "\n",
    "        return resp_dict if response_format else resp\n",
    "\n",
    "    def _force_chat_completion(self):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        '''\n",
    "        To fix bug where model response is blank.\n",
    "        IMPORTANT: response is added to the history\n",
    "        '''\n",
    "        resp = None\n",
    "        while resp == None or resp['content'] == '': \n",
    "            resp = GLOBAL_LLM_INSTANCE.create_chat_completion(self._history, temperature=self._temperature, repeat_penalty=self._repeat_penalty)['choices'][0]['message']\n",
    "\n",
    "        self._history += [resp]\n",
    "\n",
    "        return resp['content']\n",
    "\n",
    "    def get_hist(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a nicely-formatted string of the current history.\n",
    "        \"\"\"\n",
    "        hist = ''\n",
    "        for msg in self._history:\n",
    "            hist += f'{msg[\"role\"]} --- {msg[\"content\"]}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def reset(self, system_prompt:str='', temperature=None, repeat_penalty=None):\n",
    "        \"\"\"\n",
    "        Reset the LLM's chat history with a new system prompt.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - instructions for the LLM, or an empty string to start without a system prompt\n",
    "        temperature:float - higher = more random\n",
    "        repeat_penalty:float - higher = less repeating of output\n",
    "        \"\"\"\n",
    "        if system_prompt:\n",
    "            self._history = [{'role':'system', 'content':system_prompt}]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = LLM('''\n",
    "You are an agent in a world trying to solve the water pouring puzzle.\n",
    "There are three cups with various amounts, and you are trying to get two cups with an amount of 4 and one with zero.\n",
    "The world will be described to you, and you will have to say how you want to interact with it.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Water_pouring_puzzle\n",
    "\n",
    "# class LLMException(Exception): pass\n",
    "\n",
    "# action params are called \"idx\" to show model, but actually are dicts\n",
    "\n",
    "def transfer(cup_from_idx:int, cup_to_idx:int):\n",
    "    '''\n",
    "    Transfer as much as possible from `cup_from` to `cup_to` WITHOUT overflowing.\n",
    "    If `cup_to` is full, nothing will happen.\n",
    "    '''\n",
    "    to_transfer = min(cup_to_idx['amt_max'] - cup_to_idx['amt'], cup_from_idx['amt'])\n",
    "    cup_from_idx['amt'] -= to_transfer\n",
    "    cup_to_idx['amt'] += to_transfer\n",
    "\n",
    "def smash_cup(cup_idx: int):\n",
    "    '''\n",
    "    smash\n",
    "    '''\n",
    "    cup_idx['amt_max']=0\n",
    "    cup_idx['amt']=0\n",
    "    cup_idx['type'] = 'smashed cup'\n",
    "    \n",
    "def obj(typ, **kwargs): return {\n",
    "    'type': typ,\n",
    "    **kwargs\n",
    "}\n",
    "\n",
    "world = [\n",
    "    obj('cup', amt=8, amt_max=8),\n",
    "    obj('cup', amt=0, amt_max=5),\n",
    "    obj('cup', amt=0, amt_max=3)\n",
    "]\n",
    "\n",
    "def describe_world(world):\n",
    "    resp = ''\n",
    "    for i, obj in enumerate(world):\n",
    "        resp += f'Object idx={i}:\\n'\n",
    "        for k,v in obj.items():\n",
    "            resp += f'\\t{k}: {v}\\n'\n",
    "    return resp\n",
    "\n",
    "actions = [transfer, smash_cup]\n",
    "actions = {a.__name__: a for a in actions}\n",
    "\n",
    "def describe_action(action_name, action):\n",
    "    args = ', '.join([f'{v.name}: {v.annotation.__name__} {v.__doc__}\\n' for k,v in signature(action).parameters.items()])\n",
    "    resp = f'{action_name}({args})'\n",
    "    return resp\n",
    "\n",
    "# transfer(world[0], world[1])\n",
    "# transfer(world[1], world[2])\n",
    "# world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object idx=0:\n",
      "\ttype: cup\n",
      "\tamt: 8\n",
      "\tamt_max: 8\n",
      "Object idx=1:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 5\n",
      "Object idx=2:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 3\n",
      "\n",
      "I want to transfer 8 from cup 0 to cup 1, as it is the only way to make cup 1 have 16 and cup 0 have 0.\n",
      "Object idx=0:\n",
      "\ttype: cup\n",
      "\tamt: 3\n",
      "\tamt_max: 8\n",
      "Object idx=1:\n",
      "\ttype: cup\n",
      "\tamt: 5\n",
      "\tamt_max: 5\n",
      "Object idx=2:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 3\n",
      "\n",
      "I want to transfer 3 from cup 0 to cup 1, as it is the only way to make cup 1 have 4 and cup 0 have 0.\n",
      "Object idx=0:\n",
      "\ttype: cup\n",
      "\tamt: 3\n",
      "\tamt_max: 8\n",
      "Object idx=1:\n",
      "\ttype: cup\n",
      "\tamt: 5\n",
      "\tamt_max: 5\n",
      "Object idx=2:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 3\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Requested tokens (4409) exceed context window of 4000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m      4\u001b[0m     world_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is the state of the world:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      6\u001b[0m         describe_world(world) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWhat do you want to do? These are your options:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([describe_action(k,v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mitems()])\n\u001b[1;32m      9\u001b[0m     )\n\u001b[0;32m---> 10\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mA1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname of known action\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython list of parameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrationale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhy are you doing this? How is this getting you closer to your goal?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     actions[resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]](\u001b[38;5;241m*\u001b[39m[world[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrationale\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[3], line 49\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, prompt, role, response_format)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m'\u001b[39m:role, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m:prompt}]\n\u001b[1;32m     47\u001b[0m last_msg_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history)\n\u001b[0;32m---> 49\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_chat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m resp_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response_format:\n",
      "Cell \u001b[0;32mIn[3], line 89\u001b[0m, in \u001b[0;36mLLM._force_chat_completion\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m resp \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m: \n\u001b[0;32m---> 89\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mGLOBAL_LLM_INSTANCE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_temperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_repeat_penalty\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [resp]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1544\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \n\u001b[1;32m   1510\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_handler \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_completion_handler(\n\u001b[1;32m   1542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format\n\u001b[1;32m   1543\u001b[0m )\n\u001b[0;32m-> 1544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllama\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1551\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama_chat_format.py:350\u001b[0m, in \u001b[0;36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[0;34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    346\u001b[0m         grammar \u001b[38;5;241m=\u001b[39m llama_grammar\u001b[38;5;241m.\u001b[39mLlamaGrammar\u001b[38;5;241m.\u001b[39mfrom_string(\n\u001b[1;32m    347\u001b[0m             llama_grammar\u001b[38;5;241m.\u001b[39mJSON_GBNF, verbose\u001b[38;5;241m=\u001b[39mllama\u001b[38;5;241m.\u001b[39mverbose\n\u001b[1;32m    348\u001b[0m         )\n\u001b[0;32m--> 350\u001b[0m completion_or_chunks \u001b[38;5;241m=\u001b[39m \u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_completion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtypical_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypical_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepeat_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepeat_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtfs_z\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtfs_z\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_tau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_tau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmirostat_eta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmirostat_eta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _convert_completion_to_chat(completion_or_chunks, stream\u001b[38;5;241m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1382\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     chunks: Iterator[CreateCompletionStreamResponse] \u001b[38;5;241m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1382\u001b[0m completion: Completion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcompletion_or_chunks\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:861\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ctx\u001b[38;5;241m.\u001b[39mreset_timings()\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_tokens) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequested tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(prompt_tokens)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceed context window of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllama_cpp\u001b[38;5;241m.\u001b[39mllama_n_ctx(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    863\u001b[0m     )\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_tokens \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m max_tokens \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;66;03m# Unlimited, depending on n_ctx.\u001b[39;00m\n\u001b[1;32m    867\u001b[0m     max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompt_tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: Requested tokens (4409) exceed context window of 4000"
     ]
    }
   ],
   "source": [
    "A1.reset()\n",
    "print(describe_world(world))\n",
    "for _ in range(10):\n",
    "    world_prompt = (\n",
    "        'This is the state of the world:\\n' +\n",
    "        describe_world(world) +\n",
    "        '\\nWhat do you want to do? These are your options:\\n' +\n",
    "        '\\n'.join([describe_action(k,v) for k, v in actions.items()])\n",
    "    )\n",
    "    resp = A1(world_prompt, response_format={\n",
    "        'action': 'name of known action',\n",
    "        'parameters': 'python list of parameters',\n",
    "        'rationale': 'why are you doing this? How is this getting you closer to your goal?'\n",
    "    })\n",
    "    actions[resp['action']](*[world[i] for i in resp['parameters']])\n",
    "    print(resp['rationale'])\n",
    "    print(describe_world(world))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
