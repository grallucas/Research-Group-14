{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open_llama_3b Model Local Running\n",
    "\n",
    "First, we need to import the llama_cpp module and the IPython display module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to load the llm from the llama_cpp module and the model_path in the directory.\n",
    "This assumes that the model weights are present in the specified path. This requires calling a few commands in some command prompt\n",
    "```\n",
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "git-lfs clone https://huggingface.co/openlm-research/open_llama_3b\n",
    "```\n",
    "Then, you will need to call \n",
    "```\n",
    "python convert.py open_llama_3b\n",
    "```\n",
    "with the last parameter being the folder of the model. Then, the .gguf and pytorch_model.bin files will be in that folder. Now,\n",
    "we can move on to loading the model with the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = llama_cpp.Llama(model_path=\"open_llama_3b/ggml-model-f16.gguf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add some ground truth for the model and then a starting point for the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = '''\n",
    "Ground truth: Your name is Gustopher. You have been my best friend since we were kids.\n",
    "Only add new lines for yourself, Gustopher.\n",
    "\n",
    "User: Hello. I might ask for help later.\n",
    "Gustopher: Ok, best friend.\n",
    "'''\n",
    "\n",
    "def chat(msg):\n",
    "    global history\n",
    "    history += 'User: ' + msg + '\\n'\n",
    "    stream = llm(history, stream=True, max_tokens=20, temperature=0.6)\n",
    "    for token in stream:\n",
    "        text = token['choices'][0]['text']\n",
    "        if '\\n' in text: break\n",
    "        history += text\n",
    "        clear_output(wait=True)\n",
    "        print(history)\n",
    "    history += '\\n'\n",
    "\n",
    "while True:\n",
    "    i = input()\n",
    "    if i.strip() == '': break\n",
    "    chat(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
