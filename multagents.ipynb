{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Agents Interacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM: \n",
    "\n",
    "    def __init__(self, model_type: str, info: dict):\n",
    "        \"\"\"Sets two class variables for the history and the llm to call.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        info: dict - any information that might be needed for the model\n",
    "        model_type: str - self-explanatory\n",
    "        \"\"\"\n",
    "        self.call_llm = None\n",
    "        self.history = self._get_key('history', info)\n",
    "        match model_type:\n",
    "            case 'local':\n",
    "                path = self._get_key('path', info)\n",
    "                llm = llama_cpp.Llama(model_path=path, max_tokens=10, temperature=0.6)\n",
    "                self.update_history = lambda prompt: self.history + f'\\nUser: {prompt}\\n'\n",
    "                self.call_llm = lambda history: llm(history)\n",
    "\n",
    "            case 'OpenAPI':\n",
    "                raise NotImplementedError(\"OpenAPI TODO\")\n",
    "    \n",
    "    def _get_key(self, key: str, info: dict):\n",
    "        \"\"\"Gets the key from the dictionary or raises an exception.\n",
    "        \"\"\"\n",
    "        if key not in info:\n",
    "            raise Exception(f'You need to specify {key} for this model type.')\n",
    "        return info[key]\n",
    "            \n",
    "    def __call__(self, prompt: str):\n",
    "        \"\"\"Overriden operator to aide in ease of calling the model. For example,\n",
    "        if llm = LLM(), then you can call llm(<some string here>) and this method will be called.\n",
    "        This method will get the next model response (modeling a chat bot), add it to the history,\n",
    "        and then returns the whole history.\n",
    "        \"\"\"\n",
    "        self.history = self.update_history(prompt)\n",
    "        model_response = self.call_llm(self.history)['choices'][0]['text']\n",
    "        model_response = self._parse_response(model_response)\n",
    "        self.history += model_response\n",
    "        return self.history\n",
    "\n",
    "    def _parse_response(self, response: str):\n",
    "        \"\"\"Very specific for local model - not sure if this will be necessary in the future.\n",
    "        \"\"\"\n",
    "        answer = \"\"\n",
    "        for token in response:\n",
    "            if token == '\\n': break\n",
    "            answer += token\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the cell below gives you errors, you need to make sure the path is correct. The branch local-llm in the github has a notebook that can lead you through getting the model running on your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "llm = LLM('local', {'path':\"../open_llama_3b/ggml-model-f16.gguf\", \n",
    "                    'history':\"\"\"Ground Truth: The bot is a helpful assistant.\\n\\nUser: I might ask you for help later.\\nBot:Okay, I will help you.\"\"\"\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ground Truth: The bot is a helpful assistant.\\n\\nUser: I might ask you for help later.\\nBot:Okay, I will help you.\\nUser: Will you be helpful?\\nBot: Yes'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Will you be helpful?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ground Truth: The bot is a helpful assistant.\\n\\nUser: I might ask you for help later.\\nBot:Okay, I will help you.\\nUser: Will you be helpful?\\nBot: Yes\\nUser: Are you sure?\\nBot: I am sure.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Are you sure?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
