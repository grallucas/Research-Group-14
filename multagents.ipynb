{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Agents Interacting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import json\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOBAL_LLM_INSTANCE = None # Run once per restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: these can be agent-specific if we want\n",
    "TEMP = 0.7\n",
    "REPEAT_PENALTY = 1.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: to use other underlying models in the future: we can make this an abstract class and subclass for each model type\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, system_prompt:str=''):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        \"\"\"\n",
    "        Create or use a global LLM instance, and initialize history\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - <see LLM.reset>\n",
    "        \"\"\"\n",
    "        if GLOBAL_LLM_INSTANCE == None:\n",
    "            GLOBAL_LLM_INSTANCE = llama_cpp.Llama(model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf', n_gpu_layers=-1, verbose=0, n_ctx=4000)\n",
    "        self.reset(system_prompt)\n",
    "\n",
    "    def __call__(self, prompt:str='', role:str='user', response_format:dict=None):\n",
    "        \"\"\"\n",
    "        Elicit a response from the LLM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt:str - text to append to the history before eliciting a response, or an empty string to use the existing history without adding to it\n",
    "        role:str - the role associated with the prompt text: 'user', 'system', or 'assistant'. Ignored if prompt is None.\n",
    "        response_format:dict - a dict format to force the response to be in -- e.g., `{'to': '<who you are talking to>', 'response': '<your actual response>'}` -- or `None` for the response to be a regular string\n",
    "        \"\"\"\n",
    "\n",
    "        if response_format:\n",
    "            self._history += [{\n",
    "                'role':'user',\n",
    "                'content': 'Your next output should be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "            }]\n",
    "\n",
    "        if prompt:\n",
    "            self._history += [{'role':role, 'content':prompt}]\n",
    "\n",
    "        last_msg_idx = len(self._history)\n",
    "\n",
    "        resp = self._force_chat_completion()\n",
    "        resp_dict = None\n",
    "\n",
    "        if response_format:\n",
    "            while True:\n",
    "                try:\n",
    "                    if '}' not in resp:\n",
    "                        resp += '}'\n",
    "                    resp = resp[resp.index('{'):] # the json might be surounded by other text\n",
    "                    resp_dict = json.loads(resp)\n",
    "                    break\n",
    "                except:\n",
    "                    self._history += [{\n",
    "                        'role':'user',\n",
    "                        'content': 'Your previous output WAS NOT correctly formatted. Make sure it has necessary curly brackets and quotes. It shold be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "                    }]\n",
    "                    resp = self._force_chat_completion()\n",
    "\n",
    "                # for debug:\n",
    "                # clear_output(wait=True)\n",
    "                # print(self.get_hist())\n",
    "                print('bad json:', resp)\n",
    "\n",
    "            # remove correction messages\n",
    "            self._history = (\n",
    "                self._history[0:last_msg_idx-2] + # up to format prompt\n",
    "                self._history[last_msg_idx-1:last_msg_idx] + # user prompt\n",
    "                self._history[-1:] # final response\n",
    "            )\n",
    "\n",
    "        return resp_dict if response_format else resp\n",
    "\n",
    "    def _force_chat_completion(self):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        '''\n",
    "        To fix bug where model response is blank.\n",
    "        IMPORTANT: response is added to the history\n",
    "        '''\n",
    "        resp = None\n",
    "        while resp == None or resp['content'] == '': \n",
    "            resp = GLOBAL_LLM_INSTANCE.create_chat_completion(self._history, temperature=TEMP, repeat_penalty=REPEAT_PENALTY)['choices'][0]['message']\n",
    "\n",
    "        self._history += [resp]\n",
    "\n",
    "        return resp['content']\n",
    "\n",
    "    def get_hist(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a nicely-formatted string of the current history.\n",
    "        \"\"\"\n",
    "        hist = ''\n",
    "        for msg in self._history:\n",
    "            hist += f'{msg[\"role\"]} --- {msg[\"content\"]}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def reset(self, system_prompt:str=''):\n",
    "        \"\"\"\n",
    "        Reset the LLM's chat history with a new system prompt.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - instructions for the LLM, or an empty string to start without a system prompt\n",
    "        \"\"\"\n",
    "        if system_prompt:\n",
    "            self._history = [{'role':'system', 'content':system_prompt}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text colors\n",
    "BLK = \"\\x1b[30m\"\n",
    "RED = \"\\x1b[31m\"\n",
    "GRN = \"\\x1b[32m\"\n",
    "YEL = \"\\x1b[33m\"\n",
    "BLU = \"\\x1b[34m\"\n",
    "MAG = \"\\x1b[35m\"\n",
    "CYN = \"\\x1b[36m\"\n",
    "WHT = \"\\x1b[37m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "\n",
    "name_colors = {\n",
    "    'Jerome': YEL,\n",
    "    'Bo': GRN,\n",
    "    'Tom': CYN\n",
    "}\n",
    "\n",
    "agent_info = {\n",
    "    'Jerome': 'a mighty barbarian',\n",
    "    'Bo': 'a high-class frenchman who does not know any French',\n",
    "    'Tom': 'an argumentative and highly opinionated tech entrepreneur'\n",
    "}\n",
    "agents = dict()\n",
    "names_filtered = lambda ex: [n for n in agent_info.keys() if n != ex]\n",
    "\n",
    "for name, status in agent_info.items():\n",
    "    others = ', '.join(names_filtered(name))\n",
    "    agents[name] = LLM(f'You are named {name}. You are {status}. The user is speaking to you on behalf of multiple people ({others}). Be concise, and do not duplicate responses.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tBonjour Jerome! *adjusts monocle* How may I assist you today?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tGreetings, noble Bo! *grins* It is an honor to serve you. How may I aid you in this fine day?\n",
      "\u001b[32mBo\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tBonjour Tom! *adjusts monocle* I see you're looking sharp today. What can I help you with?\n",
      "\u001b[36mTom\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tAh, Bonjour to you as well, old chap! *adjusts own monocle* I'm afraid I can't be bothered with your trivial inquiries, what with my cutting-edge tech startup to attend to. *rolls eyes* Do tell, what brings you to my doorstep today?\n",
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, Tom old bean! *chuckles* I see you're as sharp as ever. Perhaps you could spare a moment to discuss the finer points of high society etiquette? *winks*\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tHah! *raises an eyebrow* You want to talk etiquette with me, Tom? *chuckles* I'm afraid you're barking up the wrong tree. I'm a barbarian, not some stuffy noble. But hey, I'll listen if you've got some good stories to share.\n",
      "\u001b[36mTom\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tOh, I see. Well, I suppose you're entitled to your... unconventional ways, *coughs* Jerome. But really, etiquette is the backbone of any civilized society. *adjusts monocle* Without it, we're left with a bunch of boors and buffoons running around. *shudders* Do tell, what sort of... shall we say, 'colorful' stories do you have to share?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tOh, you want to hear some wild tales from the barbarian camps? *chuckles* Let me tell you about the time I accidentally drank a whole skin of ale in one go... *gulps* Or how about the time I got into a wrestling match with a giant, mutated bear... *flexes* Yeah, that didn't end well for me... *winks*\n",
      "\u001b[36mTom\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *adjusts monocle* Well, well, well. It seems like you and I have a lot in common, Bo. *winks* I've got my own tales of debauchery and daring-do to share, if you're interested. *smirks* But I'm afraid I can't compete with your... *coughs* unique brand of barbarian adventures. *chuckles* Do tell, what other exploits have you got up your sleeve?\n",
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, Tom old bean! *grins* I'm glad you're in the mood for reminiscing. *adjusts monocle* I've got a few tales of my own to share, if you'd like to hear them. *winks* Perhaps we could share some stories and laughs together?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *grins* Sharing stories and laughter, you say? *chuckles* I'm always up for a good time, Bo. Fire away with your tales of adventure and mischief!\n",
      "\u001b[32mBo\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tExcellent! *adjusts monocle* I've got a few stories that are sure to bring a smile to your face. *winks* How about the time I accidentally turned myself into a frog? *chuckles* Or the many times I've been chased by angry mobs of villagers? *grins* I'm sure you've got some tales of your own to share as well, old chap.\n",
      "\u001b[36mTom\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tAh, I see. *adjusts monocle* Well, well, well. Turning yourself into a frog, you say? *chuckles* And chased by angry mobs, you say? *grins* My dear fellow, you're a regular Renaissance man, aren't you? *winks* I've got my own stories to share, of course. *smirks* How about the time I accidentally turned myself into a... *coughs* well, never mind. It's not quite as... *chuckles* relevant as your tales of barbarian adventures. *adjusts monocle* Do tell, what other exploits have you got up your sleeve?\n",
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, Tom old bean! *grins* I'm glad you're enjoying the tale. *adjusts monocle* I've got plenty more where that came from, if you'd like to hear them. *winks* Perhaps we could share some stories and laughs together?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *grins* Sharing stories and laughter, you say? *chuckles* I'm always up for a good time, Bo. Fire away with your tales of adventure and mischief!\n",
      "\u001b[32mBo\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tAh, Jerome old bean! *grins* I've got a few tales of my own to share, if you'd like to hear them. *winks* Perhaps we could share some stories and laughs together?\n",
      "\u001b[36mTom\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, an opportunity for barbarian bonding, you say? *grins* I'm always up for a good yarn, my friend. *adjusts monocle* Do tell, what other exploits have you got to share with the group?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tHah! *chuckles* Oh, there's plenty more where that came from. *grins* How about the time I accidentally set fire to my own hair with a lit pipe... *giggles* Or the time I tried to cook a whole deer on a tiny little campfire... *gulps* Yeah, that didn't end well for me either... *winks*\n",
      "\u001b[36mTom\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *adjusts monocle* Well, well, well. It seems like you've got a few more... shall we say, 'creative' tales to share, Jerome. *chuckles* I'm afraid I can't compete with your... *coughs* unique brand of barbarian adventures. *adjusts monocle* Do tell, what other exploits have you got up your sleeve?\n",
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, Tom old bean! *grins* I'm glad you're in the mood for more tales of adventure. *adjusts monocle* I've got a few more stories to share, if you'd like to hear them. *winks* Perhaps we could share some stories and laughs together?\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *grins* Sharing stories and laughter, you say? *chuckles* I'm always up for a good time, Bo. Fire away with your tales of adventure and mischief!\n",
      "\u001b[32mBo\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tAh, Jerome old bean! *grins* I've got a few tales of my own to share, if you'd like to hear them. *winks* Perhaps we could share some stories and laughs together?\n",
      "\u001b[36mTom\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tHah! *chuckles* Oh, there's plenty more where that came from. *grins* How about the time I accidentally set fire to my own hair with a lit pipe... *giggles* Or the time I tried to cook a whole deer on a tiny little campfire... *gulps* Yeah, that didn't end well for me either... *winks*\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tHah! *chuckles* Oh no, Tom! *giggles* Setting fire to your hair and cooking a whole deer on a tiny campfire... *gulps* That's quite the collection of mishaps you have there!\n",
      "\u001b[36mTom\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, another kindred spirit! *adjusts monocle* I see we both have a knack for... *coughs* creative problem-solving, if you will. *chuckles* Do tell, have you ever tried to cook a whole deer on a tiny little campfire? *grins* I'm afraid I couldn't quite manage it myself, but I'm sure you've got some... shall we say, 'interesting' methods up your sleeve.\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tHah! *chuckles* Oh, you want to know my secrets? *grins* Well, let me tell you, Tom old bean, I've got a few tricks up my sleeve. *winks* I've found that if you soak the deer in water for a good long while, it makes it much easier to cook on a tiny campfire. *giggles* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\n",
      "\u001b[36mTom\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *chuckles* Oh, you want to know my secrets? *grins* Well, let me tell you, Bo old bean, I've got a few tricks up my sleeve. *winks* I've found that if you soak the deer in water for a good long while, it makes it much easier to cook on a tiny campfire. *giggles* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\n",
      "\u001b[32mBo\u001b[0m -> \u001b[33mJerome\u001b[0m\t::\tAh, Tom old bean! *grins* I see you've got some culinary secrets up your sleeve. *adjusts monocle* I've found that a good marinade can make all the difference when cooking in the wilderness. *winks* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\n",
      "\u001b[33mJerome\u001b[0m -> \u001b[32mBo\u001b[0m\t::\tHah! *chuckles* Oh, you want to know my secrets? *grins* Well, let me tell you, Bo old bean, I've got a few tricks up my sleeve. *winks* I've found that if you soak the deer in water for a good long while, it makes it much easier to cook on a tiny campfire. *giggles* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\n",
      "\u001b[32mBo\u001b[0m -> \u001b[36mTom\u001b[0m\t::\tAh, Jerome old bean! *grins* I see you've got some culinary secrets up your sleeve. *adjusts monocle* I've found that a good marinade can make all the difference when cooking in the wilderness. *winks* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\n",
      "bad json:  My apologies, here is the corrected output:\n",
      "{\"to\": \"Bo\", \"response\": \"Hah! *chuckles* Oh, you want to know my secrets? *grins* Well, let me tell you, Bo old bean, I've got a few tricks up my sleeve. *winks* I've found that if you soak the deer in water for a good long while, it makes it much easier to cook on a tiny campfire. *giggles* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\"}}\n",
      "bad json:  My apologies, here is the corrected output:\n",
      "{\"to\": \"Bo\", \"response\": \"Hah! *chuckles* Oh, you want to know my secrets? *grins* Well, let me tell you, Bo old bean, I've got a few tricks up my sleeve. *winks* I've found that if you soak the deer in water for a good long while, it makes it much easier to cook on a tiny campfire. *giggles* And as for seasoning... well, let's just say I like to add a little extra flavor with some... *coughs* 'herbs and spices' I've found in the forest. *chuckles* But hey, between you and me, it's all about the presentation, right? *grins*\"}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb Cell 7\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m resp \u001b[39m=\u001b[39m resp[resp\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m{\u001b[39m\u001b[39m'\u001b[39m):] \u001b[39m# the json might be surounded by other text\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m resp_dict \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39;49mloads(resp)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/__init__.py:357\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    355\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[1;32m    356\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n\u001b[0;32m--> 357\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_decoder\u001b[39m.\u001b[39;49mdecode(s)\n\u001b[1;32m    358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/json/decoder.py:340\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m end \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(s):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mraise\u001b[39;00m JSONDecodeError(\u001b[39m\"\u001b[39m\u001b[39mExtra data\u001b[39m\u001b[39m\"\u001b[39m, s, end)\n\u001b[1;32m    341\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 532 (char 531)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb Cell 7\u001b[0m in \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m last_msg[\u001b[39m'\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     preface \u001b[39m=\u001b[39m last_msg[\u001b[39m'\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m tells you: \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m resp \u001b[39m=\u001b[39m agents[last_msg[\u001b[39m'\u001b[39;49m\u001b[39mto\u001b[39;49m\u001b[39m'\u001b[39;49m]](\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     preface \u001b[39m+\u001b[39;49m last_msg[\u001b[39m'\u001b[39;49m\u001b[39mmsg\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     response_format\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mto\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mone of \u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39m'\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(names_filtered(last_msg[\u001b[39m'\u001b[39;49m\u001b[39mto\u001b[39;49m\u001b[39m'\u001b[39;49m]))\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m'\u001b[39;49m\u001b[39myour actual response\u001b[39;49m\u001b[39m'\u001b[39;49m}\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m last_msg[\u001b[39m'\u001b[39m\u001b[39mfrom\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m last_msg[\u001b[39m'\u001b[39m\u001b[39mto\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m last_msg[\u001b[39m'\u001b[39m\u001b[39mto\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m resp[\u001b[39m'\u001b[39m\u001b[39mto\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;32m/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb Cell 7\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [{\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=52'>53</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mYour previous output WAS NOT correctly formatted. Make sure it has necessary curly brackets and quotes. It shold be formatted as this json with nothing extra: \u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m json\u001b[39m.\u001b[39mdumps(response_format)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m     }]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_force_chat_completion()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m# for debug:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m# clear_output(wait=True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m# print(self.get_hist())\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbad json:\u001b[39m\u001b[39m'\u001b[39m, resp)\n",
      "\u001b[1;32m/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb Cell 7\u001b[0m in \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=76'>77</a>\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mwhile\u001b[39;00m resp \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m resp[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m: \n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m     resp \u001b[39m=\u001b[39m GLOBAL_LLM_INSTANCE\u001b[39m.\u001b[39;49mcreate_chat_completion(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_history, temperature\u001b[39m=\u001b[39;49mTEMP, repeat_penalty\u001b[39m=\u001b[39;49mREPEAT_PENALTY)[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [resp]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgrall@dh-mgmt2.hpc.msoe.edu/home/grall/Documents/aiClub/resgroup14/Research-Group-14/multagents.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m \u001b[39mreturn\u001b[39;00m resp[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1644\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, temperature, top_p, top_k, stream, stop, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     rstop \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mstop \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(result\u001b[39m.\u001b[39mstop, \u001b[39mlist\u001b[39m) \u001b[39melse\u001b[39;00m [result\u001b[39m.\u001b[39mstop]\n\u001b[1;32m   1642\u001b[0m     stop \u001b[39m=\u001b[39m stop \u001b[39m+\u001b[39m rstop\n\u001b[0;32m-> 1644\u001b[0m completion_or_chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1645\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1646\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1647\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1648\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1649\u001b[0m     stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1650\u001b[0m     stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1651\u001b[0m     max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1652\u001b[0m     presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1653\u001b[0m     frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1654\u001b[0m     repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1655\u001b[0m     tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1656\u001b[0m     mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1657\u001b[0m     mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1658\u001b[0m     mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1659\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1660\u001b[0m     logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1661\u001b[0m     grammar\u001b[39m=\u001b[39;49mgrammar,\n\u001b[1;32m   1662\u001b[0m )\n\u001b[1;32m   1663\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_completion_to_chat(completion_or_chunks, stream\u001b[39m=\u001b[39mstream)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1451\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1449\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1450\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1451\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1014\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1012\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1014\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m   1015\u001b[0m     prompt_tokens,\n\u001b[1;32m   1016\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m   1017\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m   1018\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m   1019\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m   1020\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1021\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1022\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1023\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1024\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1025\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1026\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1027\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m   1028\u001b[0m     grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m   1029\u001b[0m ):\n\u001b[1;32m   1030\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m   1031\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:828\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    825\u001b[0m     grammar\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    827\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    829\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    830\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    831\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m         grammar\u001b[39m=\u001b[39mgrammar,\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    844\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n\u001b[1;32m    845\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:562\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    560\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[1;32m    561\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 562\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    563\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    564\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    565\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mn_tokens,\n\u001b[1;32m    566\u001b[0m     n_past\u001b[39m=\u001b[39;49mn_past,\n\u001b[1;32m    567\u001b[0m )\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama_cpp.py:1040\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m   1035\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m   1036\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     n_tokens: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[1;32m   1038\u001b[0m     n_past: Union[c_int, \u001b[39mint\u001b[39m],\n\u001b[1;32m   1039\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "last_msg = {'from': '', 'to': 'Bo', 'msg': ''}\n",
    "\n",
    "for _ in range(100):\n",
    "    preface = ''\n",
    "    if last_msg['from']:\n",
    "        preface = last_msg['from'] + ' tells you: '\n",
    "    resp = agents[last_msg['to']](\n",
    "        preface + last_msg['msg'],\n",
    "        response_format={'to':f\"one of {', '.join(names_filtered(last_msg['to']))}\", 'response':'your actual response'}\n",
    "    )\n",
    "    last_msg['from'] = last_msg['to']\n",
    "    last_msg['to'] = resp['to']\n",
    "    last_msg['msg'] = resp['response']\n",
    "\n",
    "    from_, to = last_msg['from'], last_msg['to']\n",
    "\n",
    "    print(f\"{name_colors[from_]}{from_}{RESET} -> {name_colors[to]}{to}{RESET}\\t::\\t{last_msg['msg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
