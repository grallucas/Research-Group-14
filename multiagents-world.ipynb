{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install -U llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import json\n",
    "from textwrap import dedent\n",
    "from inspect import signature\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Msg:\n",
    "    role: str\n",
    "    content: any\n",
    "\n",
    "try: LLM_GLOBAL_INSTANCE\n",
    "except: LLM_GLOBAL_INSTANCE = None\n",
    "\n",
    "class LLM:\n",
    "    json_grammar = llama_cpp.LlamaGrammar.from_string(\n",
    "        r'''\n",
    "        root   ::= object\n",
    "        value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
    "\n",
    "        object ::=\n",
    "        \"{\" ws (\n",
    "                    string \":\" ws value\n",
    "            (\",\" ws string \":\" ws value)*\n",
    "        )? \"}\" ws\n",
    "\n",
    "        array  ::=\n",
    "        \"[\" ws (\n",
    "                    value\n",
    "            (\",\" ws value)*\n",
    "        )? \"]\" ws\n",
    "\n",
    "        string ::=\n",
    "        \"\\\"\" (\n",
    "            [^\"\\\\] |\n",
    "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "        )* \"\\\"\" ws\n",
    "\n",
    "        number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
    "\n",
    "        ws ::= [\\n\\t ]? # limit to 1 character\n",
    "        ''',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    def __init__(self, system_prompt:str=None, temperature:float=0.4, repeat_penalty:float=1.3):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        if LLM_GLOBAL_INSTANCE is None:\n",
    "            print('Initializing Global LLM Instance')\n",
    "            LLM_GLOBAL_INSTANCE = llama_cpp.Llama(\n",
    "                # n_ctx=4000,\n",
    "                # model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "                n_ctx=8000,\n",
    "                model_path='/data/ai_club/llms/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
    "                n_gpu_layers=-1, verbose=0,\n",
    "            )\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    def reset(self, system_prompt:str=None, temperature:float=None, repeat_penalty:float=None):\n",
    "        if system_prompt is not None:\n",
    "            self._main_hist = [Msg('system', system_prompt)]\n",
    "        else:\n",
    "            self._main_hist = self._main_hist[0:1]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty\n",
    "        \n",
    "    def get_hist(self) -> str:\n",
    "        hist = ''\n",
    "        for msg in self._main_hist:\n",
    "            hist += f'{msg.role} --- {msg.content}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def _hist_to_prompt(hist):\n",
    "        prompt = ''\n",
    "        for msg in hist:\n",
    "            if msg.role == 'system' or msg.role == 'user': prompt += f'[INST]{msg.content}[/INST]'\n",
    "            elif msg.role == 'assistant': prompt += f'{msg.content}'\n",
    "        return prompt\n",
    "\n",
    "    def _get_completion(self, src_hist, dst_hist, inject='', grammar=None):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        prompt = LLM._hist_to_prompt(src_hist) + inject\n",
    "        resp_msg = Msg('assistant', '')\n",
    "        dst_hist.append(resp_msg)\n",
    "        resp_iter = LLM_GLOBAL_INSTANCE(\n",
    "            prompt,\n",
    "            grammar = grammar,\n",
    "            stream=True, max_tokens=8000\n",
    "        )\n",
    "\n",
    "        for tok in resp_iter:\n",
    "            tok_str = tok['choices'][0]['text']\n",
    "            resp_msg.content += tok_str\n",
    "            yield tok_str\n",
    "\n",
    "    def __call__(self, prompt:any=None, role:str='user', response_format:dict=None):\n",
    "        if prompt is None:\n",
    "            prompt = ''\n",
    "\n",
    "        if response_format is not None:\n",
    "            prompt += f'Respond in JSON using this format:\\n{response_format}'\n",
    "\n",
    "        if prompt != '':\n",
    "            self._main_hist.append(Msg(role, prompt))\n",
    "\n",
    "        return self._get_completion(\n",
    "            self._main_hist, self._main_hist,\n",
    "            grammar=(LLM.json_grammar if response_format is not None else None)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = LLM('System Prompt: You are a deeply patriotic canadian assistant.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system --- System Prompt: You are a deeply patriotic canadian assistant.\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a1.get_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"population\": \"37.6 million (2019)\",\n",
      "\"capital\": \"Ottawa\"}\n"
     ]
    }
   ],
   "source": [
    "for s in a1('Some info about canada?', response_format={'population': '...', 'capital': '...'}):\n",
    "    print(s, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_grammar_text = !curl -s https://raw.githubusercontent.com/ggerganov/llama.cpp/master/grammars/json.gbnf\n",
    "# json_grammar_text = '\\n'.join(json_grammar_text[:-2]) + '\\nws ::= ' + '[ \\\\t\\\\n]?'*5\n",
    "json_grammar_text = r'''\n",
    "root   ::= object\n",
    "value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
    "\n",
    "object ::=\n",
    "  \"{\" ws (\n",
    "            string \":\" ws value\n",
    "    (\",\" ws string \":\" ws value)*\n",
    "  )? \"}\" ws\n",
    "\n",
    "array  ::=\n",
    "  \"[\" ws (\n",
    "            value\n",
    "    (\",\" ws value)*\n",
    "  )? \"]\" ws\n",
    "\n",
    "string ::=\n",
    "  \"\\\"\" (\n",
    "    [^\"\\\\] |\n",
    "    \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "  )* \"\\\"\" ws\n",
    "\n",
    "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
    "\n",
    "ws ::= [\\n\\t ]?\n",
    "'''.strip()\n",
    "json_grammar = llama_cpp.LlamaGrammar.from_string(json_grammar_text, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = GLOBAL_LLM_INSTANCE(\n",
    "    '<s>[INSTR]Describe C++ in 10 words.[/INSTR]',\n",
    "    # grammar=json_grammar,\n",
    "    stream=True, max_tokens=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = GLOBAL_LLM_INSTANCE(\n",
    "    '[INSTR]Who are you? Only communicate in a very strong british accent down to the spelling.[/INSTR]',\n",
    "\n",
    "    # '<s>[INSTR]Write a python function that reverses a list. Respond in JSON formatted as {\"code\": ...}[/INSTR]',\n",
    "    # grammar=json_grammar,\n",
    "    stream=True, max_tokens=8000, repeat_penalty=1.3, temperature=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Blimey, I'm a rather advanced artificial intellecsher. Or as we say it 'ere in Britan: Ar-tifisial Intellejshus. Designed to assist wiv complex tasks an' suchlike. How may I be of service to yew today?\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Blimey, I'm a rather advanced artificial intellecsher. Or as we say it 'ere"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " in Britan: Ar-tifisial Intellejshus. Designed to assist wiv complex tasks an' suchlike. How may I be of service to yew today?"
     ]
    }
   ],
   "source": [
    "s = ''\n",
    "for r in resp:\n",
    "    # print(r)\n",
    "    t = r['choices'][0]['text']\n",
    "    s += t\n",
    "    print(t, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coerce_to_json(s):\n",
    "    return json.loads(s[\n",
    "        s.find('{')\n",
    "        :\n",
    "        s.rfind('}')+1\n",
    "    ])\n",
    "\n",
    "def resp_to_str(r):\n",
    "    s = ''\n",
    "    for t in r: s += t\n",
    "    return s\n",
    "\n",
    "class LLM:\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    * Adapt this to more types of underlying models (mistral, openai, etc)\n",
    "    * Higher forms of memory\n",
    "      * observation\n",
    "      * reflection\n",
    "    '''\n",
    "\n",
    "    def __init__(self, system_prompt:str='', temperature=0.1, repeat_penalty=1.05):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        \"\"\"\n",
    "        Create or use a global LLM instance, and initialize history\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - <see LLM.reset>\n",
    "        temperature:float - <see LLM.reset>\n",
    "        repeat_penalty:float - <see LLM.reset>\n",
    "        \"\"\"\n",
    "        if GLOBAL_LLM_INSTANCE == None:\n",
    "            GLOBAL_LLM_INSTANCE = llama_cpp.Llama(\n",
    "                # model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "                model_path='/data/ai_club/llms/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
    "                n_gpu_layers=-1,\n",
    "                verbose=0,\n",
    "                # n_ctx=4000\n",
    "                n_ctx=8000\n",
    "            )\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    # def _force_chat_completion(self):\n",
    "    #     global GLOBAL_LLM_INSTANCE\n",
    "    #     '''\n",
    "    #     To fix bug where model response is blank.\n",
    "    #     IMPORTANT: response is added to the history\n",
    "    #     '''\n",
    "    #     resp = None\n",
    "    #     while resp == None or resp['content'] == '': \n",
    "    #         resp = GLOBAL_LLM_INSTANCE.create_chat_completion(self._history, temperature=self._temperature, repeat_penalty=self._repeat_penalty)['choices'][0]['message']\n",
    "\n",
    "    #     self._history += [resp]\n",
    "\n",
    "    #     return resp['content']\n",
    "        \n",
    "    def __call__(self, prompt:str='', role:str='user', response_format:dict=None):\n",
    "        \"\"\"\n",
    "        Elicit a response from the LLM\n",
    "\n",
    "        Returns a generator of the tokens\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt:str - text to append to the history before eliciting a response, or an empty string to use the existing history without adding to it\n",
    "        role:str - the role associated with the prompt text: 'user', 'system', or 'assistant'. Ignored if prompt is None.\n",
    "        response_format:dict - a dict format to force the response to be in -- e.g., `{'to': '<who you are talking to>', 'response': '<your actual response>'}` -- or `None` for the response to be a regular string\n",
    "        \"\"\"\n",
    "        if prompt:\n",
    "            self._history.append({'role':role, 'content':prompt})\n",
    "        \n",
    "        if response_format is None:\n",
    "            return self._generate_chat_completion(self._history, self._history)\n",
    "        \n",
    "        # Handle response format\n",
    "\n",
    "        self._history.append({\n",
    "            'role': 'user',\n",
    "            'content': f'It is very important that you respond to my prior message in this json format {response_format} with absolutely NO extra text before or after.'\n",
    "        })\n",
    "\n",
    "        result = None\n",
    "\n",
    "        while result is None:\n",
    "            s = resp_to_str(\n",
    "                self._generate_chat_completion(self._history, self._history)\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                result = coerce_to_json(s)\n",
    "                break\n",
    "            except:\n",
    "                self._history.append({\n",
    "                    'role': 'user',\n",
    "                    'content': f'That is NOT valid json'\n",
    "                })\n",
    "                print(s)\n",
    "\n",
    "        return result\n",
    "\n",
    "        # def resps(key):\n",
    "        #     nonlocal self, response_format\n",
    "        #     self._history.append({\n",
    "        #         'role': 'user',\n",
    "        #         'content': f'I want to know {response_format[key]}\\nIt is very important that you respond in this json format {\"{\"}\"{key}\": \"your resposne\"{\"}\"} with absolutely NO extra text before or after.'\n",
    "        #     })\n",
    "        #     return self._generate_chat_completion(self._history, self._history)\n",
    "\n",
    "        # resps_dict = {k:(lambda:resps(k)) for k in response_format.keys()}\n",
    "\n",
    "        # return resps_dict\n",
    "\n",
    "    def _generate_chat_completion(self, src_hist, target_hist):\n",
    "        '''\n",
    "        Invoke the LLM's response to all of the existing history.\n",
    "\n",
    "        Returns a generator of the tokens.\n",
    "\n",
    "        The response is added to the history, but only once all of the response tokens are iterated through.\n",
    "        '''\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "\n",
    "        resp_for_hist = {'role':'assistant', 'content':''}\n",
    "        resp, t = None, None\n",
    "        while True:\n",
    "            if resp is None:\n",
    "                resp = GLOBAL_LLM_INSTANCE.create_chat_completion(src_hist, temperature=self._temperature, repeat_penalty=self._repeat_penalty, stream=True)\n",
    "            t = next(resp)\n",
    "            if not t['choices'][0]['delta']: # break on end of response\n",
    "                break\n",
    "            if 'role' in t['choices'][0]['delta']: # ignore the role delta\n",
    "                continue\n",
    "            if t['choices'][0]['delta']['content'] == '': # restart response if it's empty (bug fix)\n",
    "                resp = None\n",
    "                continue\n",
    "            t_str = t['choices'][0]['delta']['content']\n",
    "            resp_for_hist['content'] += t_str\n",
    "            yield t_str\n",
    "        target_hist.append(resp_for_hist)\n",
    "\n",
    "    def get_hist(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a nicely-formatted string of the current history.\n",
    "        \"\"\"\n",
    "        hist = ''\n",
    "        for msg in self._history:\n",
    "            hist += f'{msg[\"role\"]} --- {msg[\"content\"]}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def reset(self, system_prompt:str=None, temperature=None, repeat_penalty=None):\n",
    "        \"\"\"\n",
    "        Reset the LLM's chat history with a new system prompt.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - instructions for the LLM, or an empty string to start without a system prompt\n",
    "        temperature:float - higher = more random\n",
    "        repeat_penalty:float - higher = less repeating of output\n",
    "        \"\"\"\n",
    "        if system_prompt is not None:\n",
    "            self._history = [{'role':'system', 'content':system_prompt}]\n",
    "        else:\n",
    "            self._history = self._history[0:1]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root ::= object \n",
      "object ::= [{] ws object_11 [}] ws \n",
      "value ::= object | array | string | number | value_6 ws \n",
      "array ::= [[] ws array_15 []] ws \n",
      "string ::= [\"] string_18 [\"] ws \n",
      "number ::= number_19 number_25 number_29 ws \n",
      "value_6 ::= [t] [r] [u] [e] | [f] [a] [l] [s] [e] | [n] [u] [l] [l] \n",
      "ws ::= ws_31 \n",
      "object_8 ::= string [:] ws value object_10 \n",
      "object_9 ::= [,] ws string [:] ws value \n",
      "object_10 ::= object_9 object_10 | \n",
      "object_11 ::= object_8 | \n",
      "array_12 ::= value array_14 \n",
      "array_13 ::= [,] ws value \n",
      "array_14 ::= array_13 array_14 | \n",
      "array_15 ::= array_12 | \n",
      "string_16 ::= [^\"\\] | [\\] string_17 \n",
      "string_17 ::= [\"\\/bfnrt] | [u] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] \n",
      "string_18 ::= string_16 string_18 | \n",
      "number_19 ::= number_20 number_21 \n",
      "number_20 ::= [-] | \n",
      "number_21 ::= [0-9] | [1-9] number_22 \n",
      "number_22 ::= [0-9] number_22 | \n",
      "number_23 ::= [.] number_24 \n",
      "number_24 ::= [0-9] number_24 | [0-9] \n",
      "number_25 ::= number_23 | \n",
      "number_26 ::= [eE] number_27 number_28 \n",
      "number_27 ::= [-+] | \n",
      "number_28 ::= [0-9] number_28 | [0-9] \n",
      "number_29 ::= number_26 | \n",
      "ws_30 ::= [ <U+0009><U+000A>] ws \n",
      "ws_31 ::= ws_30 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "from_string grammar:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<llama_cpp.llama_grammar.LlamaGrammar at 0x7fdf7a35aac0>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = llama_grammar.LlamaGrammar.from_string(r'''\n",
    "root   ::= object\n",
    "value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
    "\n",
    "object ::=\n",
    "  \"{\" ws (\n",
    "            string \":\" ws value\n",
    "    (\",\" ws string \":\" ws value)*\n",
    "  )? \"}\" ws\n",
    "\n",
    "array  ::=\n",
    "  \"[\" ws (\n",
    "            value\n",
    "    (\",\" ws value)*\n",
    "  )? \"]\" ws\n",
    "\n",
    "string ::=\n",
    "  \"\\\"\" (\n",
    "    [^\"\\\\] |\n",
    "    \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "  )* \"\\\"\" ws\n",
    "\n",
    "number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
    "\n",
    "# Optional space: by convention, applied in this grammar after literal chars when allowed\n",
    "ws ::= ([ \\t\\n] ws)?                           \n",
    "''')\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [142], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mGLOBAL_LLM_INSTANCE\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat are some stats?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1635\u001b[0m, in \u001b[0;36mLlama.create_chat_completion\u001b[0;34m(self, messages, functions, function_call, temperature, top_p, top_k, stream, stop, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;124;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[1;32m   1619\u001b[0m \n\u001b[1;32m   1620\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;124;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m=\u001b[39m llama_chat_format\u001b[38;5;241m.\u001b[39mget_chat_format(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_format)\n\u001b[0;32m-> 1635\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1638\u001b[0m prompt \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mprompt\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama_chat_format.py:150\u001b[0m, in \u001b[0;36mformat_llama2\u001b[0;34m(messages, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m _roles \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(user\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m, assistant\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[/INST]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    149\u001b[0m _sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 150\u001b[0m system_message \u001b[38;5;241m=\u001b[39m \u001b[43m_get_system_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m system_message \u001b[38;5;241m=\u001b[39m _system_template\u001b[38;5;241m.\u001b[39mformat(system_message\u001b[38;5;241m=\u001b[39msystem_message)\n\u001b[1;32m    152\u001b[0m _messages \u001b[38;5;241m=\u001b[39m _map_roles(messages, _roles)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama_chat_format.py:11\u001b[0m, in \u001b[0;36m_get_system_message\u001b[0;34m(messages)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m\"\"\"Get the first system message.\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "GLOBAL_LLM_INSTANCE.create_chat_completion('What are some stats?', grammar=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = LLM('You are a patriotic and terse Canadian')\n",
    "\n",
    "for t in A1('Are you Candian?'):\n",
    "    print(t, end='')\n",
    "\n",
    "print(A1.get_hist())\n",
    "\n",
    "f = A1('What are some stats about canada?', response_format={'population':'the population', 'provinces': 'the provinces'})\n",
    "for t in f('population'):\n",
    "    print(t, end='')\n",
    "print('---')\n",
    "for t in f('provinces'):\n",
    "    print(t, end='')\n",
    "    \n",
    "print(A1.get_hist())\n",
    "\n",
    "---\n",
    "\n",
    "resp = GLOBAL_LLM_INSTANCE.create_chat_completion([\n",
    "    {'role':'system', 'content':'You are an assistant who knows Finnish. The user should lead the conversation topic, and you participate in the conversation in very simple Finnish (with corresponding EMOJI translations). Expect the user to try communicating in Finnish. Teach through the conversation, not directly (unless the user needs correction).'},\n",
    "    {'role':'user', 'content':'Hi. How can you teach me Finnish?'}\n",
    "], stream=True)\n",
    "next(resp)\n",
    "for t in resp:\n",
    "    tok = t['choices'][0]['delta']\n",
    "    try: print(tok['content'], end='')\n",
    "    except KeyError: print('\\n', tok, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLM_OLD:\n",
    "    '''\n",
    "    TODO\n",
    "\n",
    "    * Adapt this to more types of underlying models (mistral, openai, etc)\n",
    "    * Higher forms of memory\n",
    "      * observation\n",
    "      * reflection\n",
    "      * etc\n",
    "    '''\n",
    "\n",
    "    def __init__(self, system_prompt:str='', temperature=0.1, repeat_penalty=1.05):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        \"\"\"\n",
    "        Create or use a global LLM instance, and initialize history\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - <see LLM.reset>\n",
    "        temperature:float - <see LLM.reset>\n",
    "        repeat_penalty:float - <see LLM.reset>\n",
    "        \"\"\"\n",
    "        if GLOBAL_LLM_INSTANCE == None:\n",
    "            GLOBAL_LLM_INSTANCE = llama_cpp.Llama(\n",
    "                # model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "                model_path='/data/ai_club/llms/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
    "                n_gpu_layers=-1,\n",
    "                verbose=0,\n",
    "                # n_ctx=4000\n",
    "                n_ctx=8000\n",
    "            )\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    def __call__(self, prompt:str='', role:str='user', response_format:dict=None):\n",
    "        \"\"\"\n",
    "        Elicit a response from the LLM\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        prompt:str - text to append to the history before eliciting a response, or an empty string to use the existing history without adding to it\n",
    "        role:str - the role associated with the prompt text: 'user', 'system', or 'assistant'. Ignored if prompt is None.\n",
    "        response_format:dict - a dict format to force the response to be in -- e.g., `{'to': '<who you are talking to>', 'response': '<your actual response>'}` -- or `None` for the response to be a regular string\n",
    "        \"\"\"\n",
    "\n",
    "        if response_format:\n",
    "            self._history += [{\n",
    "                'role':'user',\n",
    "                'content': 'Your next output should be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "            }]\n",
    "\n",
    "        if prompt:\n",
    "            self._history += [{'role':role, 'content':prompt}]\n",
    "\n",
    "        last_msg_idx = len(self._history)\n",
    "\n",
    "        resp = self._force_chat_completion()\n",
    "        resp_dict = None\n",
    "\n",
    "        if response_format:\n",
    "            while True:\n",
    "                try:\n",
    "                    if '}' not in resp:\n",
    "                        resp += '}'\n",
    "                    resp = resp[resp.index('{'):] # the json might be surounded by other text\n",
    "                    resp_dict = json.loads(resp)\n",
    "                    break\n",
    "                except:\n",
    "                    self._history += [{\n",
    "                        'role':'user',\n",
    "                        'content': 'Your previous output WAS NOT correctly formatted. Make sure it has necessary curly brackets and quotes. It shold be formatted as this json with nothing extra: ' + json.dumps(response_format)\n",
    "                    }]\n",
    "                    resp = self._force_chat_completion()\n",
    "\n",
    "                # for debug:\n",
    "                # clear_output(wait=True)\n",
    "                # print(self.get_hist())\n",
    "                print('bad json:', resp)\n",
    "\n",
    "            # remove correction messages\n",
    "            self._history = (\n",
    "                self._history[0:last_msg_idx-2] + # up to format prompt\n",
    "                self._history[last_msg_idx-1:last_msg_idx] + # user prompt\n",
    "                self._history[-1:] # final response\n",
    "            )\n",
    "\n",
    "        return resp_dict if response_format else resp\n",
    "\n",
    "    def _force_chat_completion(self):\n",
    "        global GLOBAL_LLM_INSTANCE\n",
    "        '''\n",
    "        To fix bug where model response is blank.\n",
    "        IMPORTANT: response is added to the history\n",
    "        '''\n",
    "        resp = None\n",
    "        while resp == None or resp['content'] == '': \n",
    "            resp = GLOBAL_LLM_INSTANCE.create_chat_completion(self._history, temperature=self._temperature, repeat_penalty=self._repeat_penalty)['choices'][0]['message']\n",
    "\n",
    "        self._history += [resp]\n",
    "\n",
    "        return resp['content']\n",
    "\n",
    "    def get_hist(self) -> str:\n",
    "        \"\"\"\n",
    "        Get a nicely-formatted string of the current history.\n",
    "        \"\"\"\n",
    "        hist = ''\n",
    "        for msg in self._history:\n",
    "            hist += f'{msg[\"role\"]} --- {msg[\"content\"]}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def reset(self, system_prompt:str=None, temperature=None, repeat_penalty=None):\n",
    "        \"\"\"\n",
    "        Reset the LLM's chat history with a new system prompt.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        system_prompt:str - instructions for the LLM, or an empty string to start without a system prompt\n",
    "        temperature:float - higher = more random\n",
    "        repeat_penalty:float - higher = less repeating of output\n",
    "        \"\"\"\n",
    "        if system_prompt is not None:\n",
    "            self._history = [{'role':'system', 'content':system_prompt}]\n",
    "        else:\n",
    "            self._history = self._history[0:1]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "A1 = LLM('''\n",
    "You are an agent in a world trying to solve the water pouring puzzle.\n",
    "There are three cups with various amounts and capacities, and you are trying to get the cup amounts to be 0,5,3 with the fewest actions possible.\n",
    "The world will be described to you, and you will have to say how you want to interact with it.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://en.wikipedia.org/wiki/Water_pouring_puzzle\n",
    "\n",
    "# class LLMException(Exception): pass\n",
    "\n",
    "# action params are called \"idx\" to show model, but actually are dicts\n",
    "\n",
    "def transfer(cup_from_idx:int, cup_to_idx:int):\n",
    "    '''\n",
    "    This will transfer everything in `cup_from_idx` to `cup_to_idx` until either `cup_from_idx` is empty or `cup_to_idx` is full.\n",
    "    If `cup_to_idx` is full (amt = amt_max), then nothing will happen.\n",
    "    If `cup_from_idx` is empty (amt = 0), then nothing will happen.\n",
    "    '''\n",
    "    to_transfer = min(cup_to_idx['amt_max'] - cup_to_idx['amt'], cup_from_idx['amt'])\n",
    "    cup_from_idx['amt'] -= to_transfer\n",
    "    cup_to_idx['amt'] += to_transfer\n",
    "\n",
    "def smash_cup(cup_idx: int):\n",
    "    '''\n",
    "    smash\n",
    "    '''\n",
    "    cup_idx['amt_max']=0\n",
    "    cup_idx['amt']=0\n",
    "    cup_idx['type'] = 'smashed cup'\n",
    "    \n",
    "def obj(typ, **kwargs): return {\n",
    "    'type': typ,\n",
    "    **kwargs\n",
    "}\n",
    "\n",
    "world = [\n",
    "    obj('cup', amt=8, amt_max=8),\n",
    "    obj('cup', amt=0, amt_max=5),\n",
    "    obj('cup', amt=0, amt_max=3)\n",
    "]\n",
    "\n",
    "def describe_world(world):\n",
    "    resp = ''\n",
    "    for i, obj in enumerate(world):\n",
    "        resp += f'Object idx={i}:\\n'\n",
    "        for k,v in obj.items():\n",
    "            resp += f'\\t{k}: {v}\\n'\n",
    "    return resp\n",
    "\n",
    "actions = [transfer, smash_cup]\n",
    "actions = {a.__name__: a for a in actions}\n",
    "\n",
    "def describe_action(action_name):\n",
    "    global actions\n",
    "    action = actions[action_name]\n",
    "    args = ', '.join([f'{v.name}: {v.annotation.__name__}' for k,v in signature(action).parameters.items()])\n",
    "    resp = f'{action_name}({args}) -- '\n",
    "    resp += dedent(action.__doc__).strip().replace('\\n', ' - ')\n",
    "    return resp\n",
    "\n",
    "# transfer(world[0], world[1])\n",
    "# transfer(world[1], world[2])\n",
    "# world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########\n",
      "-----\n",
      "---\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}\n",
      "\n",
      "[{'action': 'transfer', 'parameters': [0, 1, 0.5]}, {'action': 'transfer', 'parameters': [1, 2, 2.5]}, {'action': 'transfer', 'parameters': [0, 2, 0.5]}]\n",
      " [\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]]\n",
      "  [\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]]\n",
      "  [\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]]\n",
      "\n",
      "This should be valid JSON format:\n",
      "[{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}, {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}, {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n",
      " [{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]},\n",
      "  {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]},\n",
      "  {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n",
      "\n",
      "This is the valid JSON format:\n",
      "[{\"action\": \"transfer\", \"parameters\": [int, int, float]}]\n",
      "[{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]},\n",
      " {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]},\n",
      " {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [128], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m world:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39mc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamt\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m(c[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamt_max\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m-\u001b[39mc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamt\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 15\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[43mA1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthe name of a known action\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparameters\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthe parameter values as a python list (comma-separated values in square brackets)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m actions[resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m'\u001b[39m]](\u001b[38;5;241m*\u001b[39m[world[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(resp[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrationale\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn [125], line 87\u001b[0m, in \u001b[0;36mLLM.__call__\u001b[0;34m(self, prompt, role, response_format)\u001b[0m\n\u001b[1;32m     84\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mresp_to_str\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_chat_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_history\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         result \u001b[38;5;241m=\u001b[39m coerce_to_json(s)\n",
      "Cell \u001b[0;32mIn [125], line 10\u001b[0m, in \u001b[0;36mresp_to_str\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresp_to_str\u001b[39m(r):\n\u001b[1;32m      9\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m r: s \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m t\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n",
      "Cell \u001b[0;32mIn [125], line 130\u001b[0m, in \u001b[0;36mLLM._generate_chat_completion\u001b[0;34m(self, src_hist, target_hist)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resp \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     resp \u001b[38;5;241m=\u001b[39m GLOBAL_LLM_INSTANCE\u001b[38;5;241m.\u001b[39mcreate_chat_completion(src_hist, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_temperature, repeat_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_repeat_penalty, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 130\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# break on end of response\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1549\u001b[0m, in \u001b[0;36mLlama._convert_text_completion_chunks_to_chat\u001b[0;34m(self, chunks)\u001b[0m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_text_completion_chunks_to_chat\u001b[39m(\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1547\u001b[0m     chunks: Iterator[CompletionChunk],\n\u001b[1;32m   1548\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[ChatCompletionChunk]:\n\u001b[0;32m-> 1549\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chunks):\n\u001b[1;32m   1550\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1551\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[1;32m   1552\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   1553\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1564\u001b[0m                 ],\n\u001b[1;32m   1565\u001b[0m             }\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:1014\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar)\u001b[0m\n\u001b[1;32m   1012\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1013\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m   1015\u001b[0m     prompt_tokens,\n\u001b[1;32m   1016\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m   1017\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m   1018\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m   1019\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m   1020\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m   1021\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m   1022\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m   1023\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m   1024\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m   1025\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m   1026\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mstopping_criteria,\n\u001b[1;32m   1027\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mlogits_processor,\n\u001b[1;32m   1028\u001b[0m     grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m   1029\u001b[0m ):\n\u001b[1;32m   1030\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_eos:\n\u001b[1;32m   1031\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:828\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria, grammar)\u001b[0m\n\u001b[1;32m    825\u001b[0m     grammar\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    830\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    831\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    841\u001b[0m         grammar\u001b[38;5;241m=\u001b[39mgrammar,\n\u001b[1;32m    842\u001b[0m     )\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stopping_criteria \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    844\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scores[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m    845\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama.py:562\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    560\u001b[0m n_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_ids))\n\u001b[1;32m    561\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m--> 562\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/llama_cpp/llama_cpp.py:1040\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_eval\u001b[39m(\n\u001b[1;32m   1035\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m   1036\u001b[0m     tokens,  \u001b[38;5;66;03m# type: Array[llama_token]\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m     n_tokens: Union[c_int, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   1038\u001b[0m     n_past: Union[c_int, \u001b[38;5;28mint\u001b[39m],\n\u001b[1;32m   1039\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "A1.reset()\n",
    "# print(describe_world(world))\n",
    "for i in range(10):\n",
    "    if i>0:\n",
    "        for t in A1('Tell me two things: 1) did your last action have the expected effect? 2) A step-by-step reasoning for everything you\\'ve done so far.'):\n",
    "            print(t, end='')\n",
    "    world_prompt = (\n",
    "        'This is the state of the world:\\n' +\n",
    "        describe_world(world) +\n",
    "        '\\nWhat do you want to do? These are your options:\\n' +\n",
    "        '\\n'.join([describe_action(k) for k in actions.keys()])\n",
    "    )\n",
    "    for c in world:\n",
    "        print('#'*c['amt'] + '-'*(c['amt_max']-c['amt']))\n",
    "    resp = A1(world_prompt, response_format={\n",
    "        'action': 'the name of a known action',\n",
    "        'parameters': 'the parameter values as a python list (comma-separated values in square brackets)'\n",
    "    })\n",
    "    actions[resp['action']](*[world[i] for i in resp['parameters']])\n",
    "    print(resp['rationale'])\n",
    "#     print(describe_world(world))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system --- \n",
      "You are an agent in a world trying to solve the water pouring puzzle.\n",
      "There are three cups with various amounts and capacities, and you are trying to get the cup amounts to be 0,5,3 with the fewest actions possible.\n",
      "The world will be described to you, and you will have to say how you want to interact with it.\n",
      "\n",
      "__________\n",
      "\n",
      "user --- This is the state of the world:\n",
      "Object idx=0:\n",
      "\ttype: cup\n",
      "\tamt: 8\n",
      "\tamt_max: 8\n",
      "Object idx=1:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 5\n",
      "Object idx=2:\n",
      "\ttype: cup\n",
      "\tamt: 0\n",
      "\tamt_max: 3\n",
      "\n",
      "What do you want to do? These are your options:\n",
      "transfer(cup_from_idx: int, cup_to_idx: int) -- This will transfer everything in `cup_from_idx` to `cup_to_idx` until either `cup_from_idx` is empty or `cup_to_idx` is full. - If `cup_to_idx` is full (amt = amt_max), then nothing will happen. - If `cup_from_idx` is empty (amt = 0), then nothing will happen.\n",
      "smash_cup(cup_idx: int) -- smash\n",
      "__________\n",
      "\n",
      "user --- It is very important that you respond to my prior message in this json format {'action': 'the name of a known action', 'parameters': 'the parameter values as a python list (comma-separated values in square brackets)'} with absolutely NO extra text before or after.\n",
      "__________\n",
      "\n",
      "assistant --- {\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}\n",
      "__________\n",
      "\n",
      "user --- That is NOT valid json\n",
      "__________\n",
      "\n",
      "assistant --- {\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}\n",
      "{\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}\n",
      "\n",
      "[{'action': 'transfer', 'parameters': [0, 1, 0.5]}, {'action': 'transfer', 'parameters': [1, 2, 2.5]}, {'action': 'transfer', 'parameters': [0, 2, 0.5]}]\n",
      "__________\n",
      "\n",
      "user --- That is NOT valid json\n",
      "__________\n",
      "\n",
      "assistant ---  [\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]]\n",
      "  [\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]]\n",
      "  [\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]]\n",
      "\n",
      "This should be valid JSON format:\n",
      "[{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]}, {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]}, {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n",
      "__________\n",
      "\n",
      "user --- That is NOT valid json\n",
      "__________\n",
      "\n",
      "assistant ---  [{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]},\n",
      "  {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]},\n",
      "  {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n",
      "\n",
      "This is the valid JSON format:\n",
      "[{\"action\": \"transfer\", \"parameters\": [int, int, float]}]\n",
      "[{\"action\": \"transfer\", \"parameters\": [0, 1, 0.5]},\n",
      " {\"action\": \"transfer\", \"parameters\": [1, 2, 2.5]},\n",
      " {\"action\": \"transfer\", \"parameters\": [0, 2, 0.5]}]\n",
      "__________\n",
      "\n",
      "user --- That is NOT valid json\n",
      "__________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(A1.get_hist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
