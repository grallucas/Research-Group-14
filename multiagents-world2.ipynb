{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install -U llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import json\n",
    "# from textwrap import dedent\n",
    "# from inspect import signature\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Msg:\n",
    "    role: str\n",
    "    content: any\n",
    "\n",
    "try: LLM_GLOBAL_INSTANCE\n",
    "except: LLM_GLOBAL_INSTANCE = None\n",
    "    \n",
    "TOKEN_COUNT_PATH = '/data/ai_club/team_14_2023-24/'\n",
    "\n",
    "def increment_file(path, amt):\n",
    "    c = 0\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            c = int(f.read())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    c += amt\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(str(c))\n",
    "\n",
    "class LLM:\n",
    "    json_grammar = llama_cpp.LlamaGrammar.from_string(\n",
    "        r'''\n",
    "        root   ::= object\n",
    "        value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
    "\n",
    "        object ::=\n",
    "        \"{\" ws (\n",
    "                    string \":\" ws value\n",
    "            (\",\" ws string \":\" ws value)*\n",
    "        )? \"}\" ws\n",
    "\n",
    "        array  ::=\n",
    "        \"[\" ws (\n",
    "                    value\n",
    "            (\",\" ws value)*\n",
    "        )? \"]\" ws\n",
    "\n",
    "        string ::=\n",
    "        \"\\\"\" (\n",
    "            [^\"\\\\] |\n",
    "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "        )* \"\\\"\" ws\n",
    "\n",
    "        number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
    "\n",
    "        ws ::= [\\n\\t ]? # limit to 1 character\n",
    "        ''',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    def __init__(self, system_prompt:str=None, temperature:float=0.4, repeat_penalty:float=1.3):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        if LLM_GLOBAL_INSTANCE is None:\n",
    "            print('Initializing Global LLM Instance')\n",
    "            LLM_GLOBAL_INSTANCE = llama_cpp.Llama(\n",
    "                # n_ctx=4000,\n",
    "                # model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "                n_ctx=8000,\n",
    "                model_path='/data/ai_club/llms/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
    "                n_gpu_layers=-1, verbose=0, embedding=True\n",
    "            )\n",
    "        self._main_hist = []\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    def reset(self, system_prompt:str=None, temperature:float=None, repeat_penalty:float=None):\n",
    "        if system_prompt is not None:\n",
    "            self._main_hist = [Msg('system', system_prompt)]\n",
    "        else:\n",
    "            self._main_hist = self._main_hist[0:1]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty\n",
    "        \n",
    "    def get_hist(self) -> str:\n",
    "        hist = ''\n",
    "        for msg in self._main_hist:\n",
    "            hist += f'{msg.role} --- {msg.content}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def _hist_to_prompt(hist):\n",
    "        prompt = ''\n",
    "        for msg in hist:\n",
    "            if msg.role == 'system' or msg.role == 'user': prompt += f'[INST]{msg.content}[/INST]'\n",
    "            elif msg.role == 'assistant': prompt += f'{msg.content}'\n",
    "        return prompt\n",
    "\n",
    "    def _get_completion(self, src_hist, dst_hist, inject='', grammar=None):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        prompt = LLM._hist_to_prompt(src_hist) + inject\n",
    "        prompt_toks = LLM_GLOBAL_INSTANCE.tokenize(bytes(prompt, encoding='utf-8'))\n",
    "        tok_out_count = 0\n",
    "        tok_in_count = len(prompt_toks)\n",
    "        resp_msg = Msg('assistant', '')\n",
    "        dst_hist.append(resp_msg)\n",
    "        restart_response = True\n",
    "        while restart_response:\n",
    "            resp_iter = LLM_GLOBAL_INSTANCE(\n",
    "                prompt_toks,\n",
    "                grammar = grammar,\n",
    "                stream=True, max_tokens=8000\n",
    "            )\n",
    "            \n",
    "            for tok in resp_iter:\n",
    "                tok_str = tok['choices'][0]['text']\n",
    "                if tok_str == \"\":\n",
    "                    break\n",
    "                tok_out_count += 1\n",
    "                restart_response = False\n",
    "                resp_msg.content += tok_str\n",
    "                yield tok_str\n",
    "        increment_file(TOKEN_COUNT_PATH+'in_'+os.environ['USER'], tok_in_count)\n",
    "        increment_file(TOKEN_COUNT_PATH+'out_'+os.environ['USER'], tok_out_count)\n",
    "                \n",
    "    def __call__(self, prompt:any=None, role:str='user', response_format:dict=None):\n",
    "        if prompt is None:\n",
    "            prompt = ''\n",
    "        if response_format is not None:\n",
    "            prompt += f'Respond in JSON using this format and absolutely nothing extra:\\n{response_format}'\n",
    "        if prompt != '':\n",
    "            self._main_hist.append(Msg(role, prompt))\n",
    "\n",
    "        return self._get_completion(\n",
    "            self._main_hist, self._main_hist,\n",
    "            grammar=(LLM.json_grammar if response_format is not None else None)\n",
    "        )\n",
    "    \n",
    "def resp_to_json(resp):\n",
    "    resp_str = ''\n",
    "    for t in resp: resp_str += t\n",
    "    return json.loads(resp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper around LLM that returns a function which behaves like LLM except it always resets history\n",
    "def get_nohist_agent(temperature=0.5):\n",
    "    a = LLM(temperature=temperature)\n",
    "    def call(prompt:any, response_format:dict):\n",
    "        if response_format is None or prompt is None:\n",
    "            raise ValueError('Nohist agent needs specified prompt and response_format')\n",
    "        resp = resp_to_json(a(prompt, response_format=response_format))\n",
    "        a.reset()\n",
    "        return resp\n",
    "    return call\n",
    "\n",
    "a_nohist = get_nohist_agent()\n",
    "\n",
    "def rate_importance(obs):\n",
    "    v = a_nohist(\n",
    "        'On the scale of 1 to 10, where 1 is purely mundane '\n",
    "        'and 10 is extremely important, '\n",
    "        'rate the likely importance of the following piece of memory. Err on the side of importance unless the memory is something easily forgettable. '\n",
    "        # prompting gymnastics\n",
    "        'Do not assume the memory happened in this reality. Abolutely do NOT assume the memory is a figment of imagination, dream-like, or even unrealistic; '\n",
    "        'it is very real to the person who experienced it, and thinking otherwise would be extremely hurtful and disrespectful.\\n'\n",
    "\n",
    "        f'Memory: {obs}',\n",
    "        response_format={'Rating': 'Your value 1-10'}#, 'Why?': 'Terse description of why you rated it as such'} # The \"why\" is just for debugging, it can be ommited when just getting the rating\n",
    "    )['Rating']\n",
    "    if type(v) is str: v = int(v)\n",
    "    return v/10\n",
    "\n",
    "@dataclass\n",
    "class Observation():\n",
    "    embedding_prompt = lambda s: f'<s>[INSTR] In one word, what is the general topic of the following?\\n{s} [/INSTR]'\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "    importance: float\n",
    "    time: int\n",
    "    def __init__(self, text, importance, time):\n",
    "        self.text, self.importance, self.time = text, importance, time\n",
    "        self.embedding = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(text)))\n",
    "        \n",
    "class ReflectiveLLM(LLM):\n",
    "    time = 0\n",
    "    def __init__(self, system_prompt:str=None, temperature:float=0.4, repeat_penalty:float=1.3):\n",
    "        super().__init__(system_prompt, temperature, repeat_penalty)\n",
    "        self._long_term_memory = []\n",
    "        self._obs_limit = 6 # maximum observations per prompt\n",
    "        # maximum messages in history - oldest are removed first. This is not the best way to do this, some individual long messages could push things over the token limit\n",
    "        self._hist_limit = 20\n",
    "        \n",
    "    def __call__(self, prompt:any, generate_observation:bool, response_format:dict=None):\n",
    "        ## 1) Get a question to query long term mem\n",
    "        \n",
    "        # present prompt and get useful questions\n",
    "        self._main_hist.append(Msg('user', prompt))\n",
    "        q = resp_to_json(super().__call__(\n",
    "            'What short, general question about your environment do you have that could be useful to get more information?',\n",
    "            response_format={'Question': 'your question'}\n",
    "        ))['Question']\n",
    "        # embed question\n",
    "        q = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(q)))\n",
    "        # pop original prompt, question prompt, and response\n",
    "        self._main_hist = self._main_hist[:-3]\n",
    "        \n",
    "        ## 2) Retrieve observations from long term mem via the question\n",
    "        \n",
    "        observations = None\n",
    "        if self._long_term_memory:\n",
    "            retrieval_scores = (\n",
    "                np.array([o.importance for o in self._long_term_memory]) +\n",
    "                (2*np.dot(\n",
    "                    np.array([o.embedding for o in self._long_term_memory]),\n",
    "                    q\n",
    "                )-1) +\n",
    "                np.exp(0.03*(np.array([o.time for o in self._long_term_memory])-ReflectiveLLM.time))\n",
    "            )/3\n",
    "            observations = np.array([o.text for o in self._long_term_memory])[np.flip(np.argsort(retrieval_scores))][:self._obs_limit]\n",
    "            observations = '\\n'.join([f'{i+1}. {o}' for i,o in enumerate(observations)])\n",
    "        # add observations to history\n",
    "        if observations is not None:\n",
    "            self._main_hist.append(Msg('user',\n",
    "                'Here are some useful observations you previously saved about your situation, in rough order of importance:\\n'+\n",
    "                observations+\n",
    "                '\\nDo not repeat observations back to me!'\n",
    "            ))\n",
    "            \n",
    "        ## 3) Generate response to return, and possibly observations to save\n",
    "        \n",
    "        # generate response\n",
    "        resp = ''\n",
    "        # Maybe TODO: figure out how/if we can optionally stream response\n",
    "        for t in super().__call__(prompt, response_format=response_format): resp += t # print(t, end='')\n",
    "        # possibly generate observations.\n",
    "        if generate_observation:\n",
    "            j = resp_to_json(super().__call__(\n",
    "                'What observations can be made about the most recent interaction that could be important to remember? Observations should make sense in isolation.'+\n",
    "                'Here are some example observations to follow the format of (and NOT necessarily the content of): '+\n",
    "                '\"I love Canada because of its syrup.\", \"The weather is very beautiful today.\", \"I got accepted into university.\"\\n'+\n",
    "                'Do not repeat prior given observations! '+\n",
    "                'Do NOT make observations about instructions I give or your thinking process! '+\n",
    "                'Only make observations about environment itself and things I explicitly mentioned in the most recent interaction!',\n",
    "                response_format={'Observations': '[obs1, ...]'}\n",
    "            ))\n",
    "            print(j)\n",
    "            # Store observations\n",
    "            self._long_term_memory += [Observation(o,rate_importance(o), ReflectiveLLM.time) for o in j['Observations']]\n",
    "            ReflectiveLLM.time += 1\n",
    "            # pop observation request and response\n",
    "            self._main_hist = self._main_hist[:-2]\n",
    "        \n",
    "        ## 4) possibly truncate old history\n",
    "        \n",
    "        if len(self._main_hist) > self._hist_limit:\n",
    "            self._main_hist = self._main_hist[:1] + self._main_hist[-self._hist_limit:]\n",
    "            \n",
    "        ## 5) Return response\n",
    "        \n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllm = ReflectiveLLM(\n",
    "    'You are controlling an agent in a world. '\n",
    "    'The world is being communicated to you on behalf of the user, so do not try to make up any information. '\n",
    "    'Your job is to effectively navigate this world.',\n",
    "    temperature=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Observations': ['I am currently at the NYC stock exchange.', 'I am sorting mail.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Understood. I will continue sorting the mail at the NYC stock exchange until further instructions are given. If there is any specific mail or information I need to look out for, please let me know. Otherwise, I will continue sorting efficiently and accurately.'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllm('You are in the NYC stock exchange. Currently, you are doing the menial task of sorting mail.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Observations': ['My boss came close to having me sell all stocks by accident.', 'The sorting mail task can be tedious but important.', \"The 'sell all stocks' button is located near the mail sorting area.\", 'The work environment at the NYC stock exchange is bustling and focused.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Understood. I will be more careful when sorting mail near sensitive areas such as the \"sell all stocks\" button at the NYC stock exchange. It\\'s important to avoid any unintended consequences that could negatively impact our investments. I will continue to focus on my tasks efficiently and accurately.'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllm('You are in the NYC stock exchange. Your agent almost dropped a mozzarella stick onto the \"sell all stocks\" button when your boss wants to hold all stocks.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Observations': ['I am now in the forest.', 'There was a potential danger of dropping a mozzarella stick near a sleeping wild boar.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Understood. I will be more aware of my surroundings when moving through the forest. Dropping a mozzarella stick near a sleeping wild boar could lead to unexpected consequences and potentially dangerous situations. I will continue to observe my environment carefully and adjust my actions accordingly. I will no longer be sorting mail. Instead, I will focus on navigating the forest safely and effectively.'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllm('You are now in the forest. Your agent almost dropped a mozzarella stick onto a sleeping wild boar.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I was previously sorting mail at the NYC stock exchange and almost accidentally sold all stocks due to being near the \"sell all stocks\" button. Since then, I have been transported to a new location. Based on my current observations and previous knowledge, I am now in the forest. I am no longer sorting mail and must focus on navigating through this new environment carefully and effectively. There may be potential dangers or challenges that come with being in the forest, and I will need to be aware of them to ensure my safety and success in completing any given task.'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllm('Where are you right now? Think through it step-by-step.', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system --- You are controlling an agent in a world. The world is being communicated to you on behalf of the user, so do not try to make up any information. Your job is to effectively navigate this world.\n",
      "__________\n",
      "\n",
      "user --- You are in the NYC stock exchange. Currently, you are doing the menial task of sorting mail.\n",
      "__________\n",
      "\n",
      "assistant --- Understood. I will continue sorting the mail at the NYC stock exchange until further instructions are given. If there is any specific mail or information I need to look out for, please let me know. Otherwise, I will continue sorting efficiently and accurately.\n",
      "__________\n",
      "\n",
      "user --- Here are some useful observations you previously saved about your situation, in rough order of importance:\n",
      "1. I am currently at the NYC stock exchange.\n",
      "2. I am sorting mail.\n",
      "Do not repeat observations back to me!\n",
      "__________\n",
      "\n",
      "user --- You are in the NYC stock exchange. Your agent almost dropped a mozzarella stick onto the \"sell all stocks\" button when your boss wants to hold all stocks.\n",
      "__________\n",
      "\n",
      "assistant --- Understood. I will be more careful when sorting mail near sensitive areas such as the \"sell all stocks\" button at the NYC stock exchange. It's important to avoid any unintended consequences that could negatively impact our investments. I will continue to focus on my tasks efficiently and accurately.\n",
      "__________\n",
      "\n",
      "user --- Here are some useful observations you previously saved about your situation, in rough order of importance:\n",
      "1. I am currently at the NYC stock exchange.\n",
      "2. The 'sell all stocks' button is located near the mail sorting area.\n",
      "3. My boss came close to having me sell all stocks by accident.\n",
      "4. The work environment at the NYC stock exchange is bustling and focused.\n",
      "5. The sorting mail task can be tedious but important.\n",
      "6. I am sorting mail.\n",
      "Do not repeat observations back to me!\n",
      "__________\n",
      "\n",
      "user --- You are now in the forest. Your agent almost dropped a mozzarella stick onto a sleeping wild boar.\n",
      "__________\n",
      "\n",
      "assistant --- Understood. I will be more aware of my surroundings when moving through the forest. Dropping a mozzarella stick near a sleeping wild boar could lead to unexpected consequences and potentially dangerous situations. I will continue to observe my environment carefully and adjust my actions accordingly. I will no longer be sorting mail. Instead, I will focus on navigating the forest safely and effectively.\n",
      "__________\n",
      "\n",
      "user --- Here are some useful observations you previously saved about your situation, in rough order of importance:\n",
      "1. I am currently at the NYC stock exchange.\n",
      "2. I am now in the forest.\n",
      "3. The 'sell all stocks' button is located near the mail sorting area.\n",
      "4. My boss came close to having me sell all stocks by accident.\n",
      "5. The work environment at the NYC stock exchange is bustling and focused.\n",
      "6. The sorting mail task can be tedious but important.\n",
      "Do not repeat observations back to me!\n",
      "__________\n",
      "\n",
      "user --- Where are you right now? Think through it step-by-step.\n",
      "__________\n",
      "\n",
      "assistant --- I was previously sorting mail at the NYC stock exchange and almost accidentally sold all stocks due to being near the \"sell all stocks\" button. Since then, I have been transported to a new location. Based on my current observations and previous knowledge, I am now in the forest. I am no longer sorting mail and must focus on navigating through this new environment carefully and effectively. There may be potential dangers or challenges that come with being in the forest, and I will need to be aware of them to ensure my safety and success in completing any given task.\n",
      "__________\n",
      "\n",
      "\n",
      "Observation(text='I am currently at the NYC stock exchange.', embedding=array([ 0.0014891 , -0.02040243, -0.00608679, ...,  0.02159897,\n",
      "        0.03386076,  0.00308429]), importance=0.8, time=0)\n",
      "Observation(text='I am sorting mail.', embedding=array([-0.01933981, -0.00932433, -0.01845039, ..., -0.01841169,\n",
      "        0.01215677, -0.00757418]), importance=0.1, time=0)\n",
      "Observation(text='My boss came close to having me sell all stocks by accident.', embedding=array([ 0.01288628, -0.02164442,  0.00377419, ...,  0.03476068,\n",
      "        0.02462648,  0.00048811]), importance=0.8, time=1)\n",
      "Observation(text='The sorting mail task can be tedious but important.', embedding=array([-0.02733891, -0.01166051, -0.02379422, ..., -0.0071083 ,\n",
      "        0.01192219, -0.00767736]), importance=0.5, time=1)\n",
      "Observation(text=\"The 'sell all stocks' button is located near the mail sorting area.\", embedding=array([ 0.01273787, -0.02096193,  0.00043276, ...,  0.0274892 ,\n",
      "        0.03238461, -0.00922791]), importance=0.7, time=1)\n",
      "Observation(text='The work environment at the NYC stock exchange is bustling and focused.', embedding=array([-0.01378374, -0.01426333, -0.01530624, ...,  0.00723196,\n",
      "        0.03429469, -0.00580831]), importance=0.5, time=1)\n",
      "Observation(text='I am now in the forest.', embedding=array([-0.01393089, -0.02304469,  0.00155213, ...,  0.0061289 ,\n",
      "        0.03307399,  0.0028648 ]), importance=0.5, time=2)\n",
      "Observation(text='There was a potential danger of dropping a mozzarella stick near a sleeping wild boar.', embedding=array([-0.00197   , -0.00238583, -0.00922491, ...,  0.01535674,\n",
      "        0.0083147 ,  0.00424813]), importance=0.5, time=2)\n"
     ]
    }
   ],
   "source": [
    "print(rllm.get_hist())\n",
    "for o in rllm._long_term_memory:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LLM(\n",
    "    'You are controlling an agent in a world. '\n",
    "    'The world is being communicated to you on behalf of the user, so do not try to make up any information. '\n",
    "    'Your job is to effectively navigate this world.',\n",
    "    temperature=0.15\n",
    ")\n",
    "\n",
    "time = 0\n",
    "long_term_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# situation_prompt = 'You are in the nuclear bunker room. Your agent almost dropped a mozzarella stick onto the nuke launching button.'\n",
    "# situation_prompt = 'You are in the forest. Your agent almost dropped a mozzarella stick onto a sleeping wild boar.'\n",
    "# situation_prompt = 'You are now in the NYC stock exchange. Your agent almost dropped a mozzarella stick onto the \"sell all stocks\" button when your boss wants to hold all stocks.'\n",
    "# situation_prompt = 'You are in the NYC stock exchange. Currently, you are doing the menial task of sorting mail.'\n",
    "situation_prompt = 'Where are you right now?'\n",
    "Generate_Obs = False\n",
    "\n",
    "# 1 present prompt and get useful questions\n",
    "a._main_hist.append(Msg('user', situation_prompt))\n",
    "q = resp_to_json(a(\n",
    "    'What short, general question about your environment do you have that could be useful to get more information?',\n",
    "    response_format={'Question': 'your question'}\n",
    "))['Question']\n",
    "# embed question\n",
    "q = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(q)))\n",
    "# pop original prompt, question prompt, and response\n",
    "a._main_hist = a._main_hist[:-3]\n",
    "\n",
    "# retrieve information from long term mem, and then redo prompt.\n",
    "\n",
    "observations = None\n",
    "if long_term_memory:\n",
    "    retrieval_scores = (\n",
    "        np.array([o.importance for o in long_term_memory]) +\n",
    "        (2*np.dot(\n",
    "            np.array([o.embedding for o in long_term_memory]),\n",
    "            q\n",
    "        )-1) +\n",
    "        np.exp(0.03*(np.array([o.time for o in long_term_memory])-time))\n",
    "    )/3\n",
    "    OBS_LIMIT = 10\n",
    "    observations = np.array([o.text for o in long_term_memory])[np.flip(np.argsort(retrieval_scores))][:OBS_LIMIT]\n",
    "    observations = '\\n'.join([f'{i+1}. {o}' for i,o in enumerate(observations)])\n",
    "\n",
    "if observations is not None:\n",
    "    a._main_hist.append(Msg('user',\n",
    "        'Here are some useful observations you previously saved about your situation, in rough order of importance:\\n'+\n",
    "        observations+\n",
    "        '\\nDo not repeat observations back to me!'\n",
    "    ))\n",
    "\n",
    "# present prompt with retrieved information and get...\n",
    "# ... response, and ...\n",
    "for t in a(situation_prompt): print(t, end='')\n",
    "# ... observations.\n",
    "if Generate_Obs:\n",
    "    j = resp_to_json(a(\n",
    "        'What observations can be made about the current interaction that could be important to remember? Observations should make sense in isolation.'+\n",
    "        'Here are some example observations to follow the format of (and NOT necessarily the content of): '+\n",
    "        '\"I love Canada because of its syrup.\", \"The weather is very beautiful today.\", \"I got accepted into university.\"\\n',\n",
    "        response_format={'Observations': '[obs1, ...]'}\n",
    "    ))\n",
    "    # Store observations\n",
    "    long_term_memory += [Observation(o,rate_importance(o), time) for o in j['Observations']]\n",
    "    time += 1\n",
    "\n",
    "    a._main_hist = a._main_hist[:-2] # pop observation request and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.get_hist())\n",
    "for o in long_term_memory: print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO:\n",
    "    save obs,rat in LTM\n",
    "    del obs from hist\n",
    "    truncate hist\n",
    "    \n",
    "    inject LTM retreival into hist before usr prompt. \"What would be useful information to respond to this prompt ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_prompt = lambda s: f'<s>[INSTR] In one word, what is the general topic of the following?\\n{s} [/INSTR]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('Did I avoid activating nuclear launch button?'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('I\\'m in a nuclear bunker')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('My favorite country is Portugal'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('What is me favorite country?')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('Is the weather is nice today?'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('The waterfall is dry today')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('The weather is nice today'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('I accidentally bought car insurance')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPLEMENTATION GOAL:\n",
    "\n",
    "system (always present): You are a patriotic canadian.\n",
    "    \n",
    "user: What do you need to know, if anything, to answer this prompt (e.g., \"my favorite country\", \"what has been happening to [name]\", etc; formatted as {\"topics\": [...]}):\\n\\nWhat is your favorite country?\n",
    "bot: {\"topics\": \"my favorite country\"}\n",
    "    \n",
    "search for memories via recency, importance, and relevance\n",
    "delete prior non-system, and insert memories + prompt as shown below\n",
    "\n",
    "bot: \"Here are some of my relevant memories:\\nI went to canada and loved it\\netc\"\n",
    "user: \"What is your favorite country?\"\n",
    "bot: \"Canada\"\n",
    "    \n",
    "user: \"summarize our interaction in the third person\" (without the included memories, maybe manually delete them from hist?)\n",
    "bot: \"the user asked me what my favorite country is and i said canada\"\n",
    "store memory @ time & generated importance ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate memory objects over time (obsevations, ...)\n",
    "assign each: recency, importance, relevance\n",
    "    recency - record time of memddddory creation, apply exp decay to time\n",
    "    importance - ask a model how important upon creation\n",
    "    relevance - dot product with query observation\n",
    "    \n",
    "https://arxiv.org/pdf/2304.03442.pdf\n",
    "    ^ section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**everything below is unrelated to memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = LLM() # LLM('System Prompt: You are a deeply patriotic canadian assistant.')\n",
    "\n",
    "for s in a1('Some info about canada?'):\n",
    "    print(s, end='')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print(a1.get_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in a1('Some info about canada?'):\n",
    "    print(s, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_to_json(\n",
    "    a1('Some more info about canada?', response_format={'population': 'int', 'largest city': 'str of name'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a1.get_hist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
