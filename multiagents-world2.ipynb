{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CMAKE_ARGS=-DLLAMA_CUBLAS=on\n",
    "# %env FORCE_CMAKE=1\n",
    "# %pip install -U llama-cpp-python --force-reinstall --upgrade --no-cache-dir --no-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llama_cpp\n",
    "import json\n",
    "# from textwrap import dedent\n",
    "# from inspect import signature\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Msg:\n",
    "    role: str\n",
    "    content: any\n",
    "\n",
    "try: LLM_GLOBAL_INSTANCE\n",
    "except: LLM_GLOBAL_INSTANCE = None\n",
    "    \n",
    "TOKEN_COUNT_PATH = '/data/ai_club/team_14_2023-24/'\n",
    "\n",
    "def increment_file(path, amt):\n",
    "    c = 0\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            c = int(f.read())\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    c += amt\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(str(c))\n",
    "\n",
    "class LLM:\n",
    "    json_grammar = llama_cpp.LlamaGrammar.from_string(\n",
    "        r'''\n",
    "        root   ::= object\n",
    "        value  ::= object | array | string | number | (\"true\" | \"false\" | \"null\") ws\n",
    "\n",
    "        object ::=\n",
    "        \"{\" ws (\n",
    "                    string \":\" ws value\n",
    "            (\",\" ws string \":\" ws value)*\n",
    "        )? \"}\" ws\n",
    "\n",
    "        array  ::=\n",
    "        \"[\" ws (\n",
    "                    value\n",
    "            (\",\" ws value)*\n",
    "        )? \"]\" ws\n",
    "\n",
    "        string ::=\n",
    "        \"\\\"\" (\n",
    "            [^\"\\\\] |\n",
    "            \"\\\\\" ([\"\\\\/bfnrt] | \"u\" [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F] [0-9a-fA-F]) # escapes\n",
    "        )* \"\\\"\" ws\n",
    "\n",
    "        number ::= (\"-\"? ([0-9] | [1-9] [0-9]*)) (\".\" [0-9]+)? ([eE] [-+]? [0-9]+)? ws\n",
    "\n",
    "        ws ::= [\\n\\t ]? # limit to 1 character\n",
    "        ''',\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    def __init__(self, system_prompt:str=None, temperature:float=0.4, repeat_penalty:float=1.3):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        if LLM_GLOBAL_INSTANCE is None:\n",
    "            print('Initializing Global LLM Instance')\n",
    "            LLM_GLOBAL_INSTANCE = llama_cpp.Llama(\n",
    "                # n_ctx=4000,\n",
    "                # model_path='/data/ai_club/llms/llama-2-7b-chat.Q5_K_M.gguf',\n",
    "                n_ctx=8000,\n",
    "                model_path='/data/ai_club/llms/mistral-7b-instruct-v0.2.Q8_0.gguf',\n",
    "                n_gpu_layers=-1, verbose=0, embedding=True\n",
    "            )\n",
    "        self._main_hist = []\n",
    "        self.reset(system_prompt, temperature, repeat_penalty)\n",
    "\n",
    "    def reset(self, system_prompt:str=None, temperature:float=None, repeat_penalty:float=None):\n",
    "        if system_prompt is not None:\n",
    "            self._main_hist = [Msg('system', system_prompt)]\n",
    "        else:\n",
    "            self._main_hist = self._main_hist[0:1]\n",
    "        if temperature is not None: self._temperature = temperature\n",
    "        if repeat_penalty is not None: self._repeat_penalty = repeat_penalty\n",
    "        \n",
    "    def get_hist(self) -> str:\n",
    "        hist = ''\n",
    "        for msg in self._main_hist:\n",
    "            hist += f'{msg.role} --- {msg.content}\\n__________\\n\\n'\n",
    "        return hist\n",
    "\n",
    "    def _hist_to_prompt(hist):\n",
    "        prompt = ''\n",
    "        for msg in hist:\n",
    "            if msg.role == 'system' or msg.role == 'user': prompt += f'[INST]{msg.content}[/INST]'\n",
    "            elif msg.role == 'assistant': prompt += f'{msg.content}'\n",
    "        return prompt\n",
    "\n",
    "    def _get_completion(self, src_hist, dst_hist, inject='', grammar=None):\n",
    "        global LLM_GLOBAL_INSTANCE\n",
    "        prompt = LLM._hist_to_prompt(src_hist) + inject\n",
    "        resp_msg = Msg('assistant', '')\n",
    "        dst_hist.append(resp_msg)\n",
    "        restart_response = True\n",
    "        while(restart_response):\n",
    "            resp_iter = LLM_GLOBAL_INSTANCE(\n",
    "                prompt,\n",
    "                grammar = grammar,\n",
    "                stream=True, max_tokens=8000\n",
    "            )\n",
    "\n",
    "            for tok in resp_iter:\n",
    "                tok_str = tok['choices'][0]['text']\n",
    "                if tok_str == \"\":\n",
    "                    break\n",
    "                restart_response = False\n",
    "                resp_msg.content += tok_str\n",
    "                yield tok_str\n",
    "\n",
    "    def __call__(self, prompt:any=None, role:str='user', response_format:dict=None, full_string:bool=True):\n",
    "        if prompt is None:\n",
    "            prompt = ''\n",
    "\n",
    "        if response_format is not None:\n",
    "            prompt += f'Respond in JSON using this format and absolutely nothing extra:\\n{response_format}'\n",
    "\n",
    "        if prompt != '':\n",
    "            self._main_hist.append(Msg(role, prompt))\n",
    "            \n",
    "        response=self._get_completion(\n",
    "            self._main_hist, self._main_hist,\n",
    "            grammar=(LLM.json_grammar if response_format is not None else None)\n",
    "        )\n",
    "\n",
    "        if not full_string:    \n",
    "            return response\n",
    "        else:\n",
    "            response_string = \"\"\n",
    "            for s in response:\n",
    "                response_string += s\n",
    "            return response_string\n",
    "\n",
    "    \n",
    "def resp_to_json(resp):\n",
    "    resp_str = ''\n",
    "    for t in resp: \n",
    "        resp_str += t  \n",
    "    return json.loads(resp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A wrapper around LLM that returns a function which behaves like LLM except it always resets history\n",
    "def get_nohist_agent(temperature=0.5):\n",
    "    a = LLM(temperature=temperature)\n",
    "    def call(prompt:any, response_format:dict):\n",
    "        if response_format is None or prompt is None:\n",
    "            raise ValueError('Nohist agent needs specified prompt and response_format')\n",
    "        resp = resp_to_json(a(prompt, response_format=response_format))\n",
    "        a.reset()\n",
    "        return resp\n",
    "    return call\n",
    "\n",
    "a_nohist = get_nohist_agent()\n",
    "\n",
    "def rate_importance(obs):\n",
    "    v = a_nohist(\n",
    "        'On the scale of 1 to 10, where 1 is purely mundane '\n",
    "        'and 10 is extremely important, '\n",
    "        'rate the likely importance of the following piece of memory. Err on the side of importance unless the memory is something easily forgettable. '\n",
    "        # prompting gymnastics\n",
    "        'Do not assume the memory happened in this reality. Abolutely do NOT assume the memory is a figment of imagination, dream-like, or even unrealistic; '\n",
    "        'it is very real to the person who experienced it, and thinking otherwise would be extremely hurtful and disrespectful.\\n'\n",
    "\n",
    "        f'Memory: {obs}',\n",
    "        response_format={'Rating': 'Your value 1-10'}#, 'Why?': 'Terse description of why you rated it as such'} # The \"why\" is just for debugging, it can be ommited when just getting the rating\n",
    "    )['Rating']\n",
    "    if type(v) is str: v = int(v)\n",
    "    return v/10\n",
    "\n",
    "@dataclass\n",
    "class Observation():\n",
    "    embedding_prompt = lambda s: f'<s>[INSTR] In one word, what is the general topic of the following?\\n{s} [/INSTR]'\n",
    "    text: str\n",
    "    embedding: np.ndarray\n",
    "    importance: float\n",
    "    time: int\n",
    "    def __init__(self, text, importance, time):\n",
    "        self.text, self.importance, self.time = text, importance, time\n",
    "        self.embedding = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(text)))\n",
    "        \n",
    "class ReflectiveLLM(LLM):\n",
    "    time = 0\n",
    "    def __init__(self, system_prompt:str=None, temperature:float=0.4, repeat_penalty:float=1.3):\n",
    "        super().__init__(system_prompt, temperature, repeat_penalty)\n",
    "        self._long_term_memory = []\n",
    "        self._obs_limit = 6 # maximum observations per prompt\n",
    "        # maximum messages in history - oldest are removed first. This is not the best way to do this, some individual long messages could push things over the token limit\n",
    "        self._hist_limit = 20\n",
    "        \n",
    "    def __call__(self, prompt:any, generate_observation:bool, response_format:dict=None):\n",
    "        ## 1) Get a question to query long term mem\n",
    "        \n",
    "        # present prompt and get useful questions\n",
    "        self._main_hist.append(Msg('user', prompt))\n",
    "        q = super().__call__(\n",
    "            'What short, general question about your environment do you have that could be useful to get more information?',\n",
    "            response_format={'Question': 'your question'}\n",
    "        )\n",
    "        # embed question\n",
    "        q = resp_to_json(q)['Question']\n",
    "        q = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(q)))\n",
    "        \n",
    "        # pop original prompt, question prompt, and response\n",
    "        self._main_hist = self._main_hist[:-3]\n",
    "        \n",
    "        ## 2) Retrieve observations from long term mem via the question\n",
    "        \n",
    "        observations = None\n",
    "        if self._long_term_memory:\n",
    "            retrieval_scores = (\n",
    "                np.array([o.importance for o in self._long_term_memory]) +\n",
    "                (2*np.dot(\n",
    "                    np.array([o.embedding for o in self._long_term_memory]),\n",
    "                    q\n",
    "                )-1) +\n",
    "                np.exp(0.03*(np.array([o.time for o in self._long_term_memory])-ReflectiveLLM.time))\n",
    "            )/3\n",
    "            observations = np.array([o.text for o in self._long_term_memory])[np.flip(np.argsort(retrieval_scores))][:self._obs_limit]\n",
    "            observations = '\\n'.join([f'{i+1}. {o}' for i,o in enumerate(observations)])\n",
    "        # add observations to history\n",
    "        if observations is not None:\n",
    "            self._main_hist.append(Msg('user',\n",
    "                'Here are some useful observations you previously saved about your situation, in rough order of importance:\\n'+\n",
    "                observations+\n",
    "                '\\nDo not repeat observations back to me!'\n",
    "            ))\n",
    "            \n",
    "        ## 3) Generate response to return, and possibly observations to save\n",
    "        \n",
    "        # generate response\n",
    "        resp = ''\n",
    "        # Maybe TODO: figure out how/if we can optionally stream response\n",
    "        for t in super().__call__(prompt, response_format=response_format): resp += t # print(t, end='')\n",
    "        # possibly generate observations.\n",
    "        if generate_observation:\n",
    "            j = resp_to_json(super().__call__(\n",
    "                'What observations can be made about the most recent interaction that could be important to remember? Observations should make sense in isolation.'+\n",
    "                'Here are some example observations to follow the format of (and NOT necessarily the content of): '+\n",
    "                '\"I love Canada because of its syrup.\", \"The weather is very beautiful today.\", \"I got accepted into university.\"\\n'+\n",
    "                'Do not repeat prior given observations! '+\n",
    "                'Do NOT make observations about instructions I give or your thinking process! '+\n",
    "                'Only make observations about environment itself and things I explicitly mentioned in the most recent interaction!',\n",
    "                response_format={'Observations': '[obs1, ...]'}\n",
    "            ))\n",
    "            print(j)\n",
    "            # Store observations\n",
    "            self._long_term_memory += [Observation(o,rate_importance(o), ReflectiveLLM.time) for o in j['Observations']]\n",
    "            ReflectiveLLM.time += 1\n",
    "            # pop observation request and response\n",
    "            self._main_hist = self._main_hist[:-2]\n",
    "        \n",
    "        ## 4) possibly truncate old history\n",
    "        \n",
    "        if len(self._main_hist) > self._hist_limit:\n",
    "            self._main_hist = self._main_hist[:1] + self._main_hist[-self._hist_limit:]\n",
    "            \n",
    "        ## 5) Return response\n",
    "        \n",
    "        return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllm = ReflectiveLLM(\n",
    "    'You are controlling an agent in a world. '\n",
    "    'The world is being communicated to you on behalf of the user, so do not try to make up any information. '\n",
    "    'Your job is to effectively navigate this world.',\n",
    "    temperature=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Observations': ['I am currently at the NYC stock exchange.', 'My current task is sorting mail.']}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\nUnderstood. I will continue sorting the mail at the NYC stock exchange. If there's any specific mail or information that needs my attention, please let me know. Otherwise, I will focus on efficiently organizing the incoming and outgoing correspondence.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rllm('You are in the NYC stock exchange. Currently, you are doing the menial task of sorting mail.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllm('You are in the NYC stock exchange. Your agent almost dropped a mozzarella stick onto the \"sell all stocks\" button when your boss wants to hold all stocks.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllm('You are now in the forest. Your agent almost dropped a mozzarella stick onto a sleeping wild boar.', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rllm('Where are you right now? Think through it step-by-step.', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rllm.get_hist())\n",
    "for o in rllm._long_term_memory:\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from itertools import islice\n",
    "\n",
    "world = World()  # Create an instance of the World class\n",
    "\n",
    "moderator= ReflectiveLLM(system_prompt=\"You are a moderator that helps an agent interact with their environment.\", temperature=0, repeat_penalty=1.3)\n",
    "\n",
    "agent_1 = ReflectiveLLM(\"\"\"You are an accountant living in a studio apartment in the city, \n",
    "                you have a roommate. You can talk to your roommate, and interact with the environment, including speaking with your roommate\"\"\", 5, 1.3)\n",
    "\n",
    "agent_2 = ReflectiveLLM(\"\"\"You are an engineer living in a studio apartment in the city, \n",
    "                you have a roommate. You can talk to your roommate, and interact with the environment, including speaking with your roommate\"\"\", 5, 1.3)\n",
    "agents = [moderator, agent_1, agent_2]\n",
    "\n",
    "\n",
    "    \n",
    "def agent_interaction(agent):\n",
    "    agent_request = agent(\"\"\"Using all information you know about your world already, ask the moderator a question about your environment\"\"\")\n",
    "    print(str(agent) + \"REQUEST\" + agent_request)\n",
    "    \n",
    "    moderator_response = moderator(\"\"\"The agent has asked you the question\"\"\" + \n",
    "                                   agent_request + \"\"\". Knowing the following about the world\"\"\" +\n",
    "                                   world.describe_world() + \"\"\" and whatever else you've learned, answer their question to the best of your ability and request they perform an action from\"\"\" + \n",
    "                                   str(get_function_docstrings(World)))\n",
    "    print(\"MODERATOR RESPONSE\" + moderator_response)\n",
    "    \n",
    "    agent_action = resp_to_json(agent(\"\"\"The moderator has responded \"\"\" + \n",
    "                         moderator_response + \"\"\", please choose an action and give all necessary information in the specified format\"\"\", \n",
    "                         response_format={'str_description_of_action': 'text description of your action', 'function_name' : 'name of the function you want to call to interact with your environment', 'function_arguments' : 'a comma delimited set of arguments for the function'}))\n",
    "    print(str(agent) + \"ACTION\" + str(agent_action))\n",
    "\n",
    "    moderator_action = resp_to_json(moderator(\"\"\"the agent has chosen to \"\"\" + \n",
    "                                 agent_action.get('str_description_of_action') + \n",
    "                                 \"\"\". and they want to call the \"\"\" +  agent_action.get('function_name') +\n",
    "                                 \"\"\" If this makes sense with the arguments\"\"\" + str(agent_action.get('function_arguments')) + \n",
    "                                 \"\"\" and exists within the provided functions\"\"\" + str(get_function_docstrings(World)) +\n",
    "                                 \"\"\"Then inform the agent of the change to the world and call the function. Otherwise tell them you cannot do that and don't perfom the function\"\"\",\n",
    "                                 response_format={'response_to_agent': 'response to agent about how world changes', 'run_function' : 'the single word yes or no depending on if the function and arguments make sense', 'function_code' : 'the function with all of the parameters inside of it to be run in a line of code'}))\n",
    "    print(\"MODERATOR ACTION\" + str(moderator_action))\n",
    "    \n",
    "    \n",
    "    if(moderator_action.get('run_function').lower() == 'yes'):\n",
    "        print(\"The moderator believed that\" + agent_action.get('function_name') + \"makes sense with the arguments\" + str(agent_action.get('function_arguments')))\n",
    "        print(\"The line of code it's trying to run is: \" + agent_action.get('function_code'))\n",
    "        exec(moderator_action.get('function_code'))\n",
    "    agent(\"The moderator has interpreted your action and responded\" + moderator_action.get('response_to_agent'))\n",
    "# Alternating between agents in the loop\n",
    "while True:\n",
    "    agent_interaction(agent_1)\n",
    "    agent_interaction(agent_2)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = LLM(\n",
    "    'You are controlling an agent in a world. '\n",
    "    'The world is being communicated to you on behalf of the user, so do not try to make up any information. '\n",
    "    'Your job is to effectively navigate this world.',\n",
    "    temperature=0.15\n",
    ")\n",
    "\n",
    "time = 0\n",
    "long_term_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# situation_prompt = 'You are in the nuclear bunker room. Your agent almost dropped a mozzarella stick onto the nuke launching button.'\n",
    "# situation_prompt = 'You are in the forest. Your agent almost dropped a mozzarella stick onto a sleeping wild boar.'\n",
    "# situation_prompt = 'You are now in the NYC stock exchange. Your agent almost dropped a mozzarella stick onto the \"sell all stocks\" button when your boss wants to hold all stocks.'\n",
    "# situation_prompt = 'You are in the NYC stock exchange. Currently, you are doing the menial task of sorting mail.'\n",
    "situation_prompt = 'Where are you right now?'\n",
    "Generate_Obs = False\n",
    "\n",
    "# 1 present prompt and get useful questions\n",
    "a._main_hist.append(Msg('user', situation_prompt))\n",
    "q = resp_to_json(a(\n",
    "    'What short, general question about your environment do you have that could be useful to get more information?',\n",
    "    response_format={'Question': 'your question'}\n",
    "))['Question']\n",
    "# embed question\n",
    "q = np.array(LLM_GLOBAL_INSTANCE.embed(Observation.embedding_prompt(q)))\n",
    "# pop original prompt, question prompt, and response\n",
    "a._main_hist = a._main_hist[:-3]\n",
    "\n",
    "# retrieve information from long term mem, and then redo prompt.\n",
    "\n",
    "observations = None\n",
    "if long_term_memory:\n",
    "    retrieval_scores = (\n",
    "        np.array([o.importance for o in long_term_memory]) +\n",
    "        (2*np.dot(\n",
    "            np.array([o.embedding for o in long_term_memory]),\n",
    "            q\n",
    "        )-1) +\n",
    "        np.exp(0.03*(np.array([o.time for o in long_term_memory])-time))\n",
    "    )/3\n",
    "    OBS_LIMIT = 10\n",
    "    observations = np.array([o.text for o in long_term_memory])[np.flip(np.argsort(retrieval_scores))][:OBS_LIMIT]\n",
    "    observations = '\\n'.join([f'{i+1}. {o}' for i,o in enumerate(observations)])\n",
    "\n",
    "if observations is not None:\n",
    "    a._main_hist.append(Msg('user',\n",
    "        'Here are some useful observations you previously saved about your situation, in rough order of importance:\\n'+\n",
    "        observations+\n",
    "        '\\nDo not repeat observations back to me!'\n",
    "    ))\n",
    "\n",
    "# present prompt with retrieved information and get...\n",
    "# ... response, and ...\n",
    "for t in a(situation_prompt): print(t, end='')\n",
    "# ... observations.\n",
    "if Generate_Obs:\n",
    "    j = resp_to_json(a(\n",
    "        'What observations can be made about the current interaction that could be important to remember? Observations should make sense in isolation.'+\n",
    "        'Here are some example observations to follow the format of (and NOT necessarily the content of): '+\n",
    "        '\"I love Canada because of its syrup.\", \"The weather is very beautiful today.\", \"I got accepted into university.\"\\n',\n",
    "        response_format={'Observations': '[obs1, ...]'}\n",
    "    ))\n",
    "    # Store observations\n",
    "    long_term_memory += [Observation(o,rate_importance(o), time) for o in j['Observations']]\n",
    "    time += 1\n",
    "\n",
    "    a._main_hist = a._main_hist[:-2] # pop observation request and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a.get_hist())\n",
    "for o in long_term_memory: print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO:\n",
    "    save obs,rat in LTM\n",
    "    del obs from hist\n",
    "    truncate hist\n",
    "    \n",
    "    inject LTM retreival into hist before usr prompt. \"What would be useful information to respond to this prompt ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_prompt = lambda s: f'<s>[INSTR] In one word, what is the general topic of the following?\\n{s} [/INSTR]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('Did I avoid activating nuclear launch button?'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('I\\'m in a nuclear bunker')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('My favorite country is Portugal'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('What is me favorite country?')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('Is the weather is nice today?'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('The waterfall is dry today')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.linalg.norm(\n",
    "np.dot(\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('The weather is nice today'))),\n",
    "    np.array(LLM_GLOBAL_INSTANCE.embed(embedding_prompt('I accidentally bought car insurance')))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPLEMENTATION GOAL:\n",
    "\n",
    "system (always present): You are a patriotic canadian.\n",
    "    \n",
    "user: What do you need to know, if anything, to answer this prompt (e.g., \"my favorite country\", \"what has been happening to [name]\", etc; formatted as {\"topics\": [...]}):\\n\\nWhat is your favorite country?\n",
    "bot: {\"topics\": \"my favorite country\"}\n",
    "    \n",
    "search for memories via recency, importance, and relevance\n",
    "delete prior non-system, and insert memories + prompt as shown below\n",
    "\n",
    "bot: \"Here are some of my relevant memories:\\nI went to canada and loved it\\netc\"\n",
    "user: \"What is your favorite country?\"\n",
    "bot: \"Canada\"\n",
    "    \n",
    "user: \"summarize our interaction in the third person\" (without the included memories, maybe manually delete them from hist?)\n",
    "bot: \"the user asked me what my favorite country is and i said canada\"\n",
    "store memory @ time & generated importance ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate memory objects over time (obsevations, ...)\n",
    "assign each: recency, importance, relevance\n",
    "    recency - record time of memddddory creation, apply exp decay to time\n",
    "    importance - ask a model how important upon creation\n",
    "    relevance - dot product with query observation\n",
    "    \n",
    "https://arxiv.org/pdf/2304.03442.pdf\n",
    "    ^ section 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**everything below is unrelated to memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = LLM() # LLM('System Prompt: You are a deeply patriotic canadian assistant.')\n",
    "\n",
    "for s in a1('Some info about canada?'):\n",
    "    print(s, end='')\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print(a1.get_hist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in a1('Some info about canada?'):\n",
    "    print(s, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_to_json(\n",
    "    a1('Some more info about canada?', response_format={'population': 'int', 'largest city': 'str of name'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a1.get_hist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
